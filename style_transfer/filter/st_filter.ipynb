{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d550968-f68a-4e27-bfc8-b762329e5e2c",
   "metadata": {},
   "source": [
    "### Style Transfer Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff7a303-a923-4881-815a-c628fca579b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gscratch/argon/hjung10/miniconda3/envs/jupyter-notebook/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import lime\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE, Laplace, Vocabulary\n",
    "import seaborn as sns\n",
    "from evaluate import load\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from scipy.stats import pearsonr\n",
    "import subprocess\n",
    "\n",
    "random.seed(42)\n",
    "#import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "088d0d12-e13a-4670-83b9-8b50747b9037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "269218f3-b7d8-48ee-bfd8-6a13a8e5d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage(idx=\"\"):\n",
    "    free, available = torch.cuda.mem_get_info()\n",
    "    print(f\"{idx} Free:{free/1000000000:.3f}GB\\tAvailable:{available/1000000000:.3f}GB\")\n",
    "#get_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885629b2-4f1a-4a65-b892-bf2d3417af83",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7646fa06-a171-4237-a23e-a211ae43a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the LLama3-generated synthetic data\n",
    "SYNTHETIC_DATA_PATH_LLAMA3 = \"/gscratch/argon/stelli/reddit_norm/style_transfer/data/output/llama3/\"\n",
    "SAVE_DICTIONARY_PATH = '/gscratch/argon/hjung10/norm_discovery_project/code/synthetic_data_detection/'\n",
    "\n",
    "# for preprocessing filter\n",
    "MEDIA_LINK_FITLERED_ID = '/gscratch/argon/stelli/reddit_norm/upvote_prediction/processed_upvotes_media_edit/'\n",
    "PREPROCESSING_FILTER_DIR = '/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/filters/preprocessing_filter/passed_filter/'   # passed_filter -> data that passed filter and is going to the next filters\n",
    "PREPROCESSING_FILTERED_DIR = '/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/filters/preprocessing_filter/filtered/'  # filtered -> data that has been filtered\n",
    "\n",
    "# Lexical filter\n",
    "LINGUISTIC_FILTER_DIR = '/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/filters/lexical_filter/passed_filter/'\n",
    "LINGUISTIC_FILTERED_DIR= '/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/filters/lexical_filter/filtered/'\n",
    "\n",
    "# fluency filter\n",
    "PERPLEXITY_FORMATTED_DIR = '/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/fluency/dialoGPT_formatted/'  # stores all intermediate perplexity computations so we don't need to repeat compute again in the future\n",
    "FLUENCY_FILTER_DIR = '/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/filters/fluency_filter/passed_filter/'\n",
    "FLUENCY_FILTERED_DIR = '/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/filters/fluency_filter/filtered/'\n",
    "\n",
    "# content preservation filter\n",
    "CONTENT_PRES_FILTER_DIR = '/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/filters/content_preservation_filter/passed_filter/'\n",
    "CONTENT_PRES_FILTERED_DIR = '/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/filters/content_preservation_filter/filtered/'\n",
    "BERTSCORE_FORMATTED_DIR = '/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/content_preservation/bertscore-generated-formatted/' # stores all intermediate bertscore computations so we don't need to repeat compute again in the future\n",
    "\n",
    "# SLURM scripts directory (see valueScope/style_transfer/filter/scripts/ directory)\n",
    "SCRIPTS_DIR = '/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/scripts/'\n",
    "\n",
    "directories_llama3 = os.listdir(SYNTHETIC_DATA_PATH_LLAMA3)\n",
    "directories_llama3.remove('dedup.py')\n",
    "directories_llama3 = [jsonl for jsonl in directories_llama3 if \"_old\" not in jsonl]\n",
    "\n",
    "# subreddits and norm dimension\n",
    "subreddits = ['askmen', 'askwomen', 'asktransgender', 'askscience', 'shittyaskscience', 'asksciencediscussion', 'democrats', 'republican',\n",
    "             'libertarian', 'stocks', 'pennystocks', 'wallstreetbets', 'wallstreetbetsnew']\n",
    "norm_dimensions = ['formality', 'humor', 'sarcasm', 'politeness', 'supportiveness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063984af-bb81-4472-8a06-fc2c996c4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_phrases = ['I apologize, but','not able to fulfill this request','cannot fulfill your request', \"I cannot provide a rewritten\"]\n",
    "split_phrases = ['\"\\n\\nOriginal Comment', '\"\\n\\nThis', '\"\\n\\nRating', '\\n\\nRating', '\"\\n\\nPlease', '\"\\n\\nNote', '\"\\n\\nRewritten', '\"\\n\\nOriginal', '\"\\n\\nRemember', '\"\\n\\nI rewrote', '\"\\n\\nI hope', '\"\\n\\nI rated', '\"\\n\\nI Changed', '\"\\n\\nI kept', '\"\\n\\nI tried', '\"\\n\\nThe original', '\"\\n\\nI\\'ve']\n",
    "split_phrases_after = ['\\n\\nMy answer: \"']\n",
    "def filter_comment(comment):\n",
    "    # filter out abstains\n",
    "    if any([phrase in comment for phrase in filter_phrases]):\n",
    "        return None\n",
    "\n",
    "    # to filter out instances where the model just generates \"NOOOOOOOOOOOO...\"\n",
    "    if \"NOOOOOOOOOOOOOOOOOOOOOOOO\" in comment: \n",
    "        return None\n",
    "\n",
    "    # filter out unnecessary phrasess\n",
    "    for split_phrase in split_phrases:\n",
    "        if split_phrase in comment:\n",
    "            return comment.split(split_phrase)[0]\n",
    "    if \"Here\\'s the rewritten\" in comment or 'Here is the rewritten' in comment or \"Here\\'s a\" in comment or 'Here is a':\n",
    "        if \"\\n\\n\" in comment:\n",
    "            index = comment.find(\"\\n\\n\")\n",
    "            return comment[index+2:]\n",
    "        if \"\\n\" in comment:\n",
    "            index = comment.find(\"\\n\")\n",
    "            return comment[index:]\n",
    "    for split_phrase in split_phrases_after:\n",
    "        if split_phrase in comment:\n",
    "            return comment.split(split_phrase)[1]\n",
    "    return comment\n",
    "\n",
    "# simple string preprocessing; standardizes quotations to use \"\" as the model\n",
    "#can add various single/double quotes in their response\n",
    "def standardize_quotations(comment):\n",
    "    if comment == None:\n",
    "        return comment\n",
    "\n",
    "    comment_cleaned = \"\"\n",
    "    if comment[0] == '\"' and comment[len(comment) - 1] == '\"':\n",
    "        comment_cleaned = comment[1:len(comment)-1]\n",
    "    elif comment[0] == \"'\" and comment[len(comment) - 1] == \"'\":\n",
    "        comment_cleaned = comment[1:len(comment)-1]\n",
    "    else:\n",
    "        comment_cleaned = comment\n",
    "\n",
    "    comment_single_quotation = ''\n",
    "    for char in comment_cleaned:\n",
    "        comment_single_quotation += char\n",
    "\n",
    "    return comment_single_quotation\n",
    "\n",
    "# preprocess each style transfer generations\n",
    "def process_data(df):\n",
    "    _data = []\n",
    "    for _, row in df.iterrows():\n",
    "        original_id = row['id']\n",
    "        _data.append({'id': original_id+\"-0\", 'id-original': original_id,'comment': row['original_comment'], 'rating': row['original_rating'], 'post_title': row['submission_title']})\n",
    "        for scale in ['1','2','3','4','5']:\n",
    "            filtered_comment = filter_comment(row[scale])\n",
    "            if filtered_comment is not None:\n",
    "                _data.append({'id': original_id+\"-\"+scale,'id-original':original_id, 'comment': filtered_comment, 'rating': int(scale), 'post_title': row['submission_title']})\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dada03ea-8575-49db-91c8-651b46d214bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in synthetic data from the directories\n",
    "def select_synthetic_data(PATH, directories):\n",
    "    reddit_to_dimension_comments = dict()\n",
    "    reddit_to_longest = dict()\n",
    "    \n",
    "    for jsonl_file in directories:   \n",
    "\n",
    "        num_duplicates = 0\n",
    "        set_added = set()\n",
    "        # load original comments data\n",
    "        with open(os.path.join(PATH, jsonl_file), \"r\") as f:\n",
    "            comments = []\n",
    "            for line in f.readlines():\n",
    "                json_line = json.loads(line)\n",
    "                id = json_line['id']\n",
    "\n",
    "                # preventing any potential duplicates in our data\n",
    "                if id in set_added:\n",
    "                    num_duplicates += 1\n",
    "                    continue\n",
    "                set_added.add(id)\n",
    "                comments.append(json_line)\n",
    "\n",
    "            # select the first 50K comments\n",
    "            reddit_to_dimension_comments[jsonl_file] = comments[:50000]\n",
    "            print(jsonl_file)\n",
    "            print(num_duplicates)\n",
    "            print(len(reddit_to_dimension_comments[jsonl_file]))\n",
    "            print(\"------\")\n",
    "\n",
    "    return reddit_to_dimension_comments#, reddit_to_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94ce1408-553c-4208-b3c5-86e1ebd17d45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "libertarian_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askscience_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askwomen_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askwomen_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "shittyaskscience_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "libertarian_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askscience_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asksciencediscussion_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "democrats_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asktransgender_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asksciencediscussion_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "stocks_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "republican_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "libertarian_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asktransgender_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "shittyaskscience_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbets_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "stocks_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askmen_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "pennystocks_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "democrats_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbetsnew_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "stocks_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asksciencediscussion_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asksciencediscussion_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askmen_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askwomen_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbetsnew_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askwomen_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asksciencediscussion_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "libertarian_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "shittyaskscience_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "shittyaskscience_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "democrats_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asktransgender_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "democrats_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "libertarian_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "republican_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asktransgender_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbets_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbetsnew_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbets_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "pennystocks_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbetsnew_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "libertarian_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbets_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askmen_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "republican_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "democrats_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askwomen_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "pennystocks_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "pennystocks_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asksciencediscussion_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "stocks_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askscience_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askmen_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askscience_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbets_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbets_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asktransgender_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "republican_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "pennystocks_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbetsnew_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "democrats_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "shittyaskscience_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "republican_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askscience_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "stocks_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askscience_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "pennystocks_humor.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "wallstreetbetsnew_formality.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "shittyaskscience_length.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "asktransgender_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "stocks_politeness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askmen_supportiveness.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "republican_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n",
      "askwomen_sarcasm.jsonl\n",
      "0\n",
      "50000\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# reading in all the style transfer data\n",
    "reddit_to_dimension_comments = select_synthetic_data(SYNTHETIC_DATA_PATH_LLAMA3, directories_llama3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652bfe66-6435-47f3-96f9-8dcf5a536e8a",
   "metadata": {},
   "source": [
    "### Preprocessing Filter\n",
    "- Filtering out edited comments, comments with only links, media/video, recent posts that may not have gotten enough time to garner community attention and upvotes, empty strings, and synthetic data that did not style transfer (e.g. same as original)\n",
    "- PREPROCESSING_FILTERED_DIR and PREPROCESSING_FILTER_DIR contains jsonl files. Both can be read as dictionaries (key being the ID, value being the first 20 characters). PREPROCESSING_FILTER_DIR represents the dictionary that passed the preprocessed filter, while the PREPROCESSING_FILTERED_DIR represents ones that were filtered out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a3205a-3684-4da7-a1eb-d9cc7d87d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERED_DIR = '/gscratch/argon/stelli/reddit_norm/upvote_prediction/data/processed_upvotes_filter/'\n",
    "\n",
    "# this function gathers all the filtered IDs\n",
    "def gather_filtered_out_ids(FILTERED_DIR):\n",
    "    filtered_ids = set()\n",
    "    for file in os.listdir(FILTERED_DIR):\n",
    "        if '.json' in file:\n",
    "            with open(FILTERED_DIR + file, 'rb') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            set_ = set()\n",
    "            for item in data['all_filters']:\n",
    "\n",
    "                # standardize the reddit comment IDs\n",
    "                if \"t1_\" in item:\n",
    "                    set_.add(item[3:])\n",
    "                else:\n",
    "                    set_.add(item)\n",
    "            filtered_ids.update(set_)\n",
    "    return filtered_ids\n",
    "\n",
    "filtered_ids = gather_filtered_out_ids(FILTERED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23e86aae-fa7c-4b30-aa26-7a3250f6986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gathers existing IDs in a set that passed the filter so we don't need to append\n",
    "# those IDs again in our files within PATH. This \n",
    "def read_existing_dictionary(PATH):\n",
    "    reddit_to_data = dict()\n",
    "    for file in os.listdir(PATH):\n",
    "        if \".jsonl\" in file:\n",
    "            set_data = set()\n",
    "            with open(PATH + file, 'r') as f:\n",
    "                for line in f:\n",
    "                    line = json.loads(line)\n",
    "                    set_data.update(line.keys())   # adding the keys (e.g. comment IDs)\n",
    "\n",
    "            reddit_to_data[file] = set_data\n",
    "    return reddit_to_data\n",
    "\n",
    "# this function gathers existing IDs as well as their mapped values in a dictionary that passed the filter\n",
    "def read_existing_dictionary_key_value(PATH):\n",
    "    reddit_to_data = dict()\n",
    "    for file in os.listdir(PATH):\n",
    "        if \".jsonl\" in file:\n",
    "            data_dict = dict()\n",
    "            with open(PATH + file, 'r') as f:\n",
    "                for line in f:\n",
    "                    line = json.loads(line)\n",
    "                    data_dict.update(line)     # adding the ID to comments together\n",
    "\n",
    "            reddit_to_data[file] = data_dict\n",
    "    return reddit_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71960bc4-fd5d-48b7-b7bf-997c9f83b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH and file_name of the file to append to for list_dict\n",
    "# previous_reddit_to_data represents the existing data so we don't append data that's already in the file\n",
    "def save_jsonl(PATH, file_name, list_dict, previous_reddit_to_data):\n",
    "    new_added = 0\n",
    "    with open(PATH + file_name, 'a') as f:\n",
    "        for dict_ in list_dict:\n",
    "            if file_name not in previous_reddit_to_data or (file_name in previous_reddit_to_data and list(dict_.keys())[0] not in previous_reddit_to_data[file_name]):\n",
    "                new_added += 1\n",
    "                json.dump(dict_, f)\n",
    "                f.write('\\n')\n",
    "    return new_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cdcdc3a-043b-4f3b-9679-f803b3b38ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing through all the synthetic comments, keeping track of which comments were filtered or not,\n",
    "# and updating our files accordingly\n",
    "def apply_preprocessed_filter():\n",
    "    total_considered = 0\n",
    "    total_filtered = 0\n",
    "    total_left = 0\n",
    "    reddit_to_preprocesed_data = read_existing_dictionary(PREPROCESSING_FILTER_DIR)\n",
    "    reddit_filtered_data = read_existing_dictionary(PREPROCESSING_FILTERED_DIR)\n",
    "    \n",
    "    # parsing through the style transferred comments\n",
    "    for reddit_json, list_synthetic in reddit_to_dimension_comments.items():\n",
    "        reddit = reddit_json.split('_')[0]\n",
    "        dimension = reddit_json.split('_')[1].split('.json')[0]\n",
    "        print(reddit)\n",
    "        print(dimension)\n",
    "        print(\"Number of IDs before preprocessed filter: \" + str(len(list_synthetic)))\n",
    "        total_considered += len(list_synthetic)\n",
    "    \n",
    "        # parsing through the comments, checking against the preprocessed filter\n",
    "        list_dict = []\n",
    "        list_filtered = []\n",
    "        for comment in list_synthetic:\n",
    "            original_id = comment['id']\n",
    "\n",
    "            # standardize the ID format\n",
    "            if \"t1_\" in original_id:\n",
    "                original_id = original_id[3:]\n",
    "            original_comment = comment['original_comment']\n",
    "    \n",
    "            # filter out edited comments, comments with only links, media/video, and recent posts that \n",
    "            # may not have gotten enough time to garner community attention and upvotes\n",
    "            id_to_comment = dict()\n",
    "            if original_id not in filtered_ids:\n",
    "                id_to_comment[original_id] = original_comment[:20]\n",
    "                list_dict.append(id_to_comment)\n",
    "            else:\n",
    "                id_to_comment[original_id] = original_comment[:20]\n",
    "                list_filtered.append(id_to_comment)\n",
    "        print(\"Number of IDs that passed the preprocessed filter: \" + str(len(list_dict)))\n",
    "        if len(list_dict) < 10000:\n",
    "            print(\"Needs more!!\")\n",
    "        total_left += len(list_dict)\n",
    "        total_filtered += len(list_filtered)\n",
    "    \n",
    "        # saving to the file\n",
    "        file_name =reddit + '_' + dimension + '.jsonl'\n",
    "        added = save_jsonl(PREPROCESSING_FILTER_DIR, file_name, list_dict, reddit_to_preprocesed_data)\n",
    "\n",
    "        # saving filtered data\n",
    "        file_name =reddit + '_' + dimension + '.jsonl'\n",
    "        added_filtered = save_jsonl(PREPROCESSING_FILTERED_DIR, file_name, list_filtered, reddit_filtered_data)\n",
    "    \n",
    "        print(\"Number of new IDs that passed filter added: \" + str(added))\n",
    "        print(\"Number of new IDs that were filtered: \" + str(added_filtered))\n",
    "        print(\"-------------------\")\n",
    "    \n",
    "    print(\"Total number of original IDs considered: \" + str(total_considered))\n",
    "    print(\"Total number of original IDs filtered: \" + str(total_filtered))\n",
    "    print(\"% of the original IDs filtered: \" + str((total_filtered) / total_considered))\n",
    "    print(\"Total number of original IDs left: \" + str(total_left))\n",
    "    print()\n",
    "    print(\"Total number of synthetic comments considered: \" + str(total_considered * 5))\n",
    "    print(\"Total number of synthetic comments filtered: \" + str(total_filtered * 5))\n",
    "    print(\"Total number of synthetic comments left: \" + str(total_left * 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8e5626d-9ab9-4b9f-a825-3ac81ea13d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 49154\n",
      "Number of new IDs that passed filter added: 49154\n",
      "Number of new IDs that were filtered: 846\n",
      "-------------------\n",
      "libertarian\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42473\n",
      "Number of new IDs that passed filter added: 42473\n",
      "Number of new IDs that were filtered: 7527\n",
      "-------------------\n",
      "askscience\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 45293\n",
      "Number of new IDs that passed filter added: 45293\n",
      "Number of new IDs that were filtered: 4707\n",
      "-------------------\n",
      "askwomen\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 48847\n",
      "Number of new IDs that passed filter added: 48847\n",
      "Number of new IDs that were filtered: 1153\n",
      "-------------------\n",
      "askwomen\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 48847\n",
      "Number of new IDs that passed filter added: 48847\n",
      "Number of new IDs that were filtered: 1153\n",
      "-------------------\n",
      "shittyaskscience\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39543\n",
      "Number of new IDs that passed filter added: 39543\n",
      "Number of new IDs that were filtered: 10457\n",
      "-------------------\n",
      "libertarian\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42455\n",
      "Number of new IDs that passed filter added: 42455\n",
      "Number of new IDs that were filtered: 7545\n",
      "-------------------\n",
      "askscience\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 45265\n",
      "Number of new IDs that passed filter added: 45265\n",
      "Number of new IDs that were filtered: 4735\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42406\n",
      "Number of new IDs that passed filter added: 42406\n",
      "Number of new IDs that were filtered: 7594\n",
      "-------------------\n",
      "democrats\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39969\n",
      "Number of new IDs that passed filter added: 39969\n",
      "Number of new IDs that were filtered: 10031\n",
      "-------------------\n",
      "asktransgender\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39535\n",
      "Number of new IDs that passed filter added: 39535\n",
      "Number of new IDs that were filtered: 10465\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42382\n",
      "Number of new IDs that passed filter added: 42382\n",
      "Number of new IDs that were filtered: 7618\n",
      "-------------------\n",
      "stocks\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 43964\n",
      "Number of new IDs that passed filter added: 43964\n",
      "Number of new IDs that were filtered: 6036\n",
      "-------------------\n",
      "republican\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44170\n",
      "Number of new IDs that passed filter added: 44170\n",
      "Number of new IDs that were filtered: 5830\n",
      "-------------------\n",
      "libertarian\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42512\n",
      "Number of new IDs that passed filter added: 42512\n",
      "Number of new IDs that were filtered: 7488\n",
      "-------------------\n",
      "asktransgender\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39147\n",
      "Number of new IDs that passed filter added: 39147\n",
      "Number of new IDs that were filtered: 10853\n",
      "-------------------\n",
      "shittyaskscience\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39542\n",
      "Number of new IDs that passed filter added: 39542\n",
      "Number of new IDs that were filtered: 10458\n",
      "-------------------\n",
      "wallstreetbets\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44752\n",
      "Number of new IDs that passed filter added: 44752\n",
      "Number of new IDs that were filtered: 5248\n",
      "-------------------\n",
      "stocks\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 43935\n",
      "Number of new IDs that passed filter added: 43935\n",
      "Number of new IDs that were filtered: 6065\n",
      "-------------------\n",
      "askmen\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 49114\n",
      "Number of new IDs that passed filter added: 49114\n",
      "Number of new IDs that were filtered: 886\n",
      "-------------------\n",
      "pennystocks\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46038\n",
      "Number of new IDs that passed filter added: 46038\n",
      "Number of new IDs that were filtered: 3962\n",
      "-------------------\n",
      "democrats\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 40038\n",
      "Number of new IDs that passed filter added: 40038\n",
      "Number of new IDs that were filtered: 9962\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46107\n",
      "Number of new IDs that passed filter added: 46107\n",
      "Number of new IDs that were filtered: 3893\n",
      "-------------------\n",
      "stocks\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 43906\n",
      "Number of new IDs that passed filter added: 43906\n",
      "Number of new IDs that were filtered: 6094\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42401\n",
      "Number of new IDs that passed filter added: 42401\n",
      "Number of new IDs that were filtered: 7599\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42398\n",
      "Number of new IDs that passed filter added: 42398\n",
      "Number of new IDs that were filtered: 7602\n",
      "-------------------\n",
      "askmen\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 49134\n",
      "Number of new IDs that passed filter added: 49134\n",
      "Number of new IDs that were filtered: 866\n",
      "-------------------\n",
      "askwomen\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 48847\n",
      "Number of new IDs that passed filter added: 48847\n",
      "Number of new IDs that were filtered: 1153\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46122\n",
      "Number of new IDs that passed filter added: 46122\n",
      "Number of new IDs that were filtered: 3878\n",
      "-------------------\n",
      "askwomen\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 48847\n",
      "Number of new IDs that passed filter added: 48847\n",
      "Number of new IDs that were filtered: 1153\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42397\n",
      "Number of new IDs that passed filter added: 42397\n",
      "Number of new IDs that were filtered: 7603\n",
      "-------------------\n",
      "libertarian\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42473\n",
      "Number of new IDs that passed filter added: 42473\n",
      "Number of new IDs that were filtered: 7527\n",
      "-------------------\n",
      "shittyaskscience\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39558\n",
      "Number of new IDs that passed filter added: 39558\n",
      "Number of new IDs that were filtered: 10442\n",
      "-------------------\n",
      "shittyaskscience\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39542\n",
      "Number of new IDs that passed filter added: 39542\n",
      "Number of new IDs that were filtered: 10458\n",
      "-------------------\n",
      "democrats\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 40147\n",
      "Number of new IDs that passed filter added: 40147\n",
      "Number of new IDs that were filtered: 9853\n",
      "-------------------\n",
      "asktransgender\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39211\n",
      "Number of new IDs that passed filter added: 39211\n",
      "Number of new IDs that were filtered: 10789\n",
      "-------------------\n",
      "democrats\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 40014\n",
      "Number of new IDs that passed filter added: 40014\n",
      "Number of new IDs that were filtered: 9986\n",
      "-------------------\n",
      "libertarian\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42013\n",
      "Number of new IDs that passed filter added: 42013\n",
      "Number of new IDs that were filtered: 7987\n",
      "-------------------\n",
      "republican\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44294\n",
      "Number of new IDs that passed filter added: 44294\n",
      "Number of new IDs that were filtered: 5706\n",
      "-------------------\n",
      "asktransgender\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39147\n",
      "Number of new IDs that passed filter added: 39147\n",
      "Number of new IDs that were filtered: 10853\n",
      "-------------------\n",
      "wallstreetbets\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44758\n",
      "Number of new IDs that passed filter added: 44758\n",
      "Number of new IDs that were filtered: 5242\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46152\n",
      "Number of new IDs that passed filter added: 46152\n",
      "Number of new IDs that were filtered: 3848\n",
      "-------------------\n",
      "wallstreetbets\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44771\n",
      "Number of new IDs that passed filter added: 44771\n",
      "Number of new IDs that were filtered: 5229\n",
      "-------------------\n",
      "pennystocks\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46086\n",
      "Number of new IDs that passed filter added: 46086\n",
      "Number of new IDs that were filtered: 3914\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46096\n",
      "Number of new IDs that passed filter added: 46096\n",
      "Number of new IDs that were filtered: 3904\n",
      "-------------------\n",
      "libertarian\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42077\n",
      "Number of new IDs that passed filter added: 42077\n",
      "Number of new IDs that were filtered: 7923\n",
      "-------------------\n",
      "wallstreetbets\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44728\n",
      "Number of new IDs that passed filter added: 44728\n",
      "Number of new IDs that were filtered: 5272\n",
      "-------------------\n",
      "askmen\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 49056\n",
      "Number of new IDs that passed filter added: 49056\n",
      "Number of new IDs that were filtered: 944\n",
      "-------------------\n",
      "republican\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44156\n",
      "Number of new IDs that passed filter added: 44156\n",
      "Number of new IDs that were filtered: 5844\n",
      "-------------------\n",
      "democrats\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 40078\n",
      "Number of new IDs that passed filter added: 40078\n",
      "Number of new IDs that were filtered: 9922\n",
      "-------------------\n",
      "askwomen\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 48847\n",
      "Number of new IDs that passed filter added: 48847\n",
      "Number of new IDs that were filtered: 1153\n",
      "-------------------\n",
      "pennystocks\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46026\n",
      "Number of new IDs that passed filter added: 46026\n",
      "Number of new IDs that were filtered: 3974\n",
      "-------------------\n",
      "pennystocks\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46001\n",
      "Number of new IDs that passed filter added: 46001\n",
      "Number of new IDs that were filtered: 3999\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 42390\n",
      "Number of new IDs that passed filter added: 42390\n",
      "Number of new IDs that were filtered: 7610\n",
      "-------------------\n",
      "stocks\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44229\n",
      "Number of new IDs that passed filter added: 44229\n",
      "Number of new IDs that were filtered: 5771\n",
      "-------------------\n",
      "askscience\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 45261\n",
      "Number of new IDs that passed filter added: 45261\n",
      "Number of new IDs that were filtered: 4739\n",
      "-------------------\n",
      "askmen\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 49017\n",
      "Number of new IDs that passed filter added: 49017\n",
      "Number of new IDs that were filtered: 983\n",
      "-------------------\n",
      "askscience\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 45248\n",
      "Number of new IDs that passed filter added: 45248\n",
      "Number of new IDs that were filtered: 4752\n",
      "-------------------\n",
      "wallstreetbets\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44731\n",
      "Number of new IDs that passed filter added: 44731\n",
      "Number of new IDs that were filtered: 5269\n",
      "-------------------\n",
      "wallstreetbets\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44734\n",
      "Number of new IDs that passed filter added: 44734\n",
      "Number of new IDs that were filtered: 5266\n",
      "-------------------\n",
      "asktransgender\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39173\n",
      "Number of new IDs that passed filter added: 39173\n",
      "Number of new IDs that were filtered: 10827\n",
      "-------------------\n",
      "republican\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44223\n",
      "Number of new IDs that passed filter added: 44223\n",
      "Number of new IDs that were filtered: 5777\n",
      "-------------------\n",
      "pennystocks\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46129\n",
      "Number of new IDs that passed filter added: 46129\n",
      "Number of new IDs that were filtered: 3871\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46103\n",
      "Number of new IDs that passed filter added: 46103\n",
      "Number of new IDs that were filtered: 3897\n",
      "-------------------\n",
      "democrats\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 40112\n",
      "Number of new IDs that passed filter added: 40112\n",
      "Number of new IDs that were filtered: 9888\n",
      "-------------------\n",
      "shittyaskscience\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39540\n",
      "Number of new IDs that passed filter added: 39540\n",
      "Number of new IDs that were filtered: 10460\n",
      "-------------------\n",
      "republican\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44270\n",
      "Number of new IDs that passed filter added: 44270\n",
      "Number of new IDs that were filtered: 5730\n",
      "-------------------\n",
      "askscience\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 45265\n",
      "Number of new IDs that passed filter added: 45265\n",
      "Number of new IDs that were filtered: 4735\n",
      "-------------------\n",
      "stocks\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 43892\n",
      "Number of new IDs that passed filter added: 43892\n",
      "Number of new IDs that were filtered: 6108\n",
      "-------------------\n",
      "askscience\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 45277\n",
      "Number of new IDs that passed filter added: 45277\n",
      "Number of new IDs that were filtered: 4723\n",
      "-------------------\n",
      "pennystocks\n",
      "humor\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46042\n",
      "Number of new IDs that passed filter added: 46042\n",
      "Number of new IDs that were filtered: 3958\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "formality\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 46102\n",
      "Number of new IDs that passed filter added: 46102\n",
      "Number of new IDs that were filtered: 3898\n",
      "-------------------\n",
      "shittyaskscience\n",
      "length\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39558\n",
      "Number of new IDs that passed filter added: 39558\n",
      "Number of new IDs that were filtered: 10442\n",
      "-------------------\n",
      "asktransgender\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 39164\n",
      "Number of new IDs that passed filter added: 39164\n",
      "Number of new IDs that were filtered: 10836\n",
      "-------------------\n",
      "stocks\n",
      "politeness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 43949\n",
      "Number of new IDs that passed filter added: 43949\n",
      "Number of new IDs that were filtered: 6051\n",
      "-------------------\n",
      "askmen\n",
      "supportiveness\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 49159\n",
      "Number of new IDs that passed filter added: 49159\n",
      "Number of new IDs that were filtered: 841\n",
      "-------------------\n",
      "republican\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 44152\n",
      "Number of new IDs that passed filter added: 44152\n",
      "Number of new IDs that were filtered: 5848\n",
      "-------------------\n",
      "askwomen\n",
      "sarcasm\n",
      "Number of IDs before preprocessed filter: 50000\n",
      "Number of IDs that passed the preprocessed filter: 48847\n",
      "Number of new IDs that passed filter added: 48847\n",
      "Number of new IDs that were filtered: 1153\n",
      "-------------------\n",
      "Total number of original IDs considered: 3900000\n",
      "Total number of original IDs filtered: 468662\n",
      "% of the original IDs filtered: 0.12016974358974358\n",
      "Total number of original IDs left: 3431338\n",
      "\n",
      "Total number of synthetic comments considered: 19500000\n",
      "Total number of synthetic comments filtered: 2343310\n",
      "Total number of synthetic comments left: 17156690\n"
     ]
    }
   ],
   "source": [
    "apply_preprocessed_filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7a9190-807e-4db0-b773-5e9a8d753fd5",
   "metadata": {},
   "source": [
    "### Lexical Filter\n",
    "- Filtering out lexical words (\"here's the rewritten comments...\"), abstains, no style transfer (e.g. synthetic comment being the same as the original), empty strings\n",
    "- Given that we are lexically preprocessing the strings, use the LINGUISTIC_FILTER_DIR from this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69756f77-4d9b-41fd-89e6-705a1f132913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_lexical_filter():\n",
    "    total_synthetic_considered = 0\n",
    "    total_synthetic_filtered = 0\n",
    "    total_synthetic_left = 0\n",
    "    reddit_to_preprocesed_data = read_existing_dictionary(PREPROCESSING_FILTER_DIR)\n",
    "    reddit_to_lexical_data = read_existing_dictionary(LINGUISTIC_FILTER_DIR)\n",
    "    reddit_to_lexical_filtered = read_existing_dictionary(LINGUISTIC_FILTERED_DIR)\n",
    "\n",
    "    # sanity check\n",
    "    total_added = 0\n",
    "    total_filtered = 0\n",
    "\n",
    "    # similar to the preprocessing filter implementation, but we decided to implement these filters\n",
    "    # separately to know how much data is being filtered at each step\n",
    "    for reddit_json, list_synthetic in reddit_to_dimension_comments.items():\n",
    "        reddit = reddit_json.split('_')[0]\n",
    "        dimension = reddit_json.split('_')[1].split('.json')[0]\n",
    "        print(reddit)\n",
    "        print(dimension)\n",
    "    \n",
    "        list_dict = []\n",
    "        list_filtered = []\n",
    "        # parsing through each set of comments (e.g. 1 original, 5 synthetic); \n",
    "        for comment in list_synthetic:\n",
    "            original_id = comment['id']\n",
    "            original_comment = comment['original_comment']\n",
    "            submission_title = comment['submission_title']\n",
    "\n",
    "            # standardize the ID format\n",
    "            if \"t1_\" in original_id:\n",
    "                original_id = original_id[3:]\n",
    "    \n",
    "            # original implementation combined both lexical and preprocessed filter together since the\n",
    "            # code is essentially the same, but we decided it would be better to separate the two filters out\n",
    "            if original_id not in reddit_to_preprocesed_data[reddit_json]:\n",
    "                continue\n",
    "    \n",
    "            # parsing through each style transfer scale\n",
    "            for scale_idx in range(1, 6):\n",
    "                synthetic_id = original_id + '-' + str(scale_idx)\n",
    "                synthetic_comment = comment[str(scale_idx)]\n",
    "                total_synthetic_considered += 1\n",
    "    \n",
    "                # filtering lexically (e.g. abstains, additional strings)\n",
    "                filtered_comment = filter_comment(synthetic_comment)\n",
    "    \n",
    "                # checking if abstains or other filtered strings, no style transfer being done, empty strings, single character generation\n",
    "                id_to_comment = dict()\n",
    "                if filtered_comment == None or original_comment == synthetic_comment or filtered_comment.strip() == \"\":\n",
    "                    id_to_comment[synthetic_id] = filtered_comment\n",
    "                    list_filtered.append(id_to_comment)     # add to list of filtered data\n",
    "                    total_synthetic_filtered += 1\n",
    "                else:\n",
    "                     # standardizing quotations to clean the data a bit better\n",
    "                    filtered_comment = standardize_quotations(filtered_comment)\n",
    "\n",
    "                    # edge case where there's only one character generation; filtering that out too\n",
    "                    if len(filtered_comment) <= 1:\n",
    "                        id_to_comment[synthetic_id] = filtered_comment\n",
    "                        list_filtered.append(id_to_comment)     # add to list of filtered data\n",
    "                        total_synthetic_filtered += 1\n",
    "                    else:\n",
    "                        # NOTE: We need to add the original comment for easier bertscore computation instead of going back into\n",
    "                        # our data and fetching them\n",
    "                        id_to_comment[synthetic_id] = (filtered_comment, original_comment)\n",
    "                        list_dict.append(id_to_comment)     # add to list of data that passed the filter\n",
    "                        total_synthetic_left += 1\n",
    "    \n",
    "        # saving to the file\n",
    "        file_name =reddit + '_' + dimension + '.jsonl'\n",
    "        added = save_jsonl(LINGUISTIC_FILTER_DIR, file_name, list_dict, reddit_to_lexical_data)\n",
    "    \n",
    "        # saving filtered data\n",
    "        file_name =reddit + '_' + dimension + '.jsonl'\n",
    "        added_filtered = save_jsonl(LINGUISTIC_FILTERED_DIR, file_name, list_filtered, reddit_to_lexical_filtered)\n",
    "    \n",
    "        print(\"Number of new IDs that passed filter added: \" + str(added))\n",
    "        print(\"Number of new IDs that were filtered: \" + str(added_filtered))\n",
    "        print(\"-------------------\")\n",
    "\n",
    "        total_added += added\n",
    "        total_filtered += added_filtered\n",
    "    \n",
    "    \n",
    "    print(\"Total number of synthetic comments considered: \" + str(total_synthetic_considered))\n",
    "    print(\"Total number of synthetic comments filtered: \" + str(total_synthetic_filtered))\n",
    "    print(\"% of the synthetic comments filtered: \" + str((total_synthetic_filtered) / total_synthetic_considered))\n",
    "    print(\"Total number of synthetic comments left: \" + str(total_synthetic_left))\n",
    "\n",
    "    print(\"total added: \" + str(total_added))\n",
    "    print(\"total filtered: \" + str(total_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18fdcd20-980c-45ac-b573-68239844e3d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen\n",
      "formality\n",
      "Number of new IDs that passed filter added: 245597\n",
      "Number of new IDs that were filtered: 173\n",
      "-------------------\n",
      "libertarian\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 212307\n",
      "Number of new IDs that were filtered: 58\n",
      "-------------------\n",
      "askscience\n",
      "humor\n",
      "Number of new IDs that passed filter added: 226435\n",
      "Number of new IDs that were filtered: 30\n",
      "-------------------\n",
      "askwomen\n",
      "length\n",
      "Number of new IDs that passed filter added: 243024\n",
      "Number of new IDs that were filtered: 1211\n",
      "-------------------\n",
      "askwomen\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 244210\n",
      "Number of new IDs that were filtered: 25\n",
      "-------------------\n",
      "shittyaskscience\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 197662\n",
      "Number of new IDs that were filtered: 53\n",
      "-------------------\n",
      "libertarian\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 212214\n",
      "Number of new IDs that were filtered: 61\n",
      "-------------------\n",
      "askscience\n",
      "length\n",
      "Number of new IDs that passed filter added: 226223\n",
      "Number of new IDs that were filtered: 102\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "length\n",
      "Number of new IDs that passed filter added: 211885\n",
      "Number of new IDs that were filtered: 145\n",
      "-------------------\n",
      "democrats\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 199791\n",
      "Number of new IDs that were filtered: 54\n",
      "-------------------\n",
      "asktransgender\n",
      "length\n",
      "Number of new IDs that passed filter added: 197220\n",
      "Number of new IDs that were filtered: 455\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 211855\n",
      "Number of new IDs that were filtered: 55\n",
      "-------------------\n",
      "stocks\n",
      "humor\n",
      "Number of new IDs that passed filter added: 219794\n",
      "Number of new IDs that were filtered: 26\n",
      "-------------------\n",
      "republican\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 220834\n",
      "Number of new IDs that were filtered: 16\n",
      "-------------------\n",
      "libertarian\n",
      "length\n",
      "Number of new IDs that passed filter added: 212282\n",
      "Number of new IDs that were filtered: 278\n",
      "-------------------\n",
      "asktransgender\n",
      "formality\n",
      "Number of new IDs that passed filter added: 195616\n",
      "Number of new IDs that were filtered: 119\n",
      "-------------------\n",
      "shittyaskscience\n",
      "humor\n",
      "Number of new IDs that passed filter added: 197663\n",
      "Number of new IDs that were filtered: 47\n",
      "-------------------\n",
      "wallstreetbets\n",
      "humor\n",
      "Number of new IDs that passed filter added: 223703\n",
      "Number of new IDs that were filtered: 57\n",
      "-------------------\n",
      "stocks\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 219662\n",
      "Number of new IDs that were filtered: 13\n",
      "-------------------\n",
      "askmen\n",
      "humor\n",
      "Number of new IDs that passed filter added: 245508\n",
      "Number of new IDs that were filtered: 62\n",
      "-------------------\n",
      "pennystocks\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 230162\n",
      "Number of new IDs that were filtered: 28\n",
      "-------------------\n",
      "democrats\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 200151\n",
      "Number of new IDs that were filtered: 39\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 230403\n",
      "Number of new IDs that were filtered: 132\n",
      "-------------------\n",
      "stocks\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 219509\n",
      "Number of new IDs that were filtered: 21\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 211975\n",
      "Number of new IDs that were filtered: 30\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "humor\n",
      "Number of new IDs that passed filter added: 211954\n",
      "Number of new IDs that were filtered: 36\n",
      "-------------------\n",
      "askmen\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 245527\n",
      "Number of new IDs that were filtered: 143\n",
      "-------------------\n",
      "askwomen\n",
      "formality\n",
      "Number of new IDs that passed filter added: 244177\n",
      "Number of new IDs that were filtered: 58\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 230497\n",
      "Number of new IDs that were filtered: 113\n",
      "-------------------\n",
      "askwomen\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 244227\n",
      "Number of new IDs that were filtered: 8\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 211957\n",
      "Number of new IDs that were filtered: 28\n",
      "-------------------\n",
      "libertarian\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 212306\n",
      "Number of new IDs that were filtered: 59\n",
      "-------------------\n",
      "shittyaskscience\n",
      "formality\n",
      "Number of new IDs that passed filter added: 197680\n",
      "Number of new IDs that were filtered: 110\n",
      "-------------------\n",
      "shittyaskscience\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 197674\n",
      "Number of new IDs that were filtered: 36\n",
      "-------------------\n",
      "democrats\n",
      "humor\n",
      "Number of new IDs that passed filter added: 200712\n",
      "Number of new IDs that were filtered: 23\n",
      "-------------------\n",
      "asktransgender\n",
      "humor\n",
      "Number of new IDs that passed filter added: 195986\n",
      "Number of new IDs that were filtered: 69\n",
      "-------------------\n",
      "democrats\n",
      "length\n",
      "Number of new IDs that passed filter added: 199589\n",
      "Number of new IDs that were filtered: 481\n",
      "-------------------\n",
      "libertarian\n",
      "humor\n",
      "Number of new IDs that passed filter added: 210001\n",
      "Number of new IDs that were filtered: 64\n",
      "-------------------\n",
      "republican\n",
      "formality\n",
      "Number of new IDs that passed filter added: 221389\n",
      "Number of new IDs that were filtered: 81\n",
      "-------------------\n",
      "asktransgender\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 195657\n",
      "Number of new IDs that were filtered: 78\n",
      "-------------------\n",
      "wallstreetbets\n",
      "formality\n",
      "Number of new IDs that passed filter added: 223674\n",
      "Number of new IDs that were filtered: 116\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "length\n",
      "Number of new IDs that passed filter added: 228016\n",
      "Number of new IDs that were filtered: 2744\n",
      "-------------------\n",
      "wallstreetbets\n",
      "length\n",
      "Number of new IDs that passed filter added: 222523\n",
      "Number of new IDs that were filtered: 1332\n",
      "-------------------\n",
      "pennystocks\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 230418\n",
      "Number of new IDs that were filtered: 12\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 230338\n",
      "Number of new IDs that were filtered: 142\n",
      "-------------------\n",
      "libertarian\n",
      "formality\n",
      "Number of new IDs that passed filter added: 210280\n",
      "Number of new IDs that were filtered: 105\n",
      "-------------------\n",
      "wallstreetbets\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 223586\n",
      "Number of new IDs that were filtered: 54\n",
      "-------------------\n",
      "askmen\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 245263\n",
      "Number of new IDs that were filtered: 17\n",
      "-------------------\n",
      "republican\n",
      "humor\n",
      "Number of new IDs that passed filter added: 220716\n",
      "Number of new IDs that were filtered: 64\n",
      "-------------------\n",
      "democrats\n",
      "formality\n",
      "Number of new IDs that passed filter added: 200313\n",
      "Number of new IDs that were filtered: 77\n",
      "-------------------\n",
      "askwomen\n",
      "humor\n",
      "Number of new IDs that passed filter added: 244211\n",
      "Number of new IDs that were filtered: 24\n",
      "-------------------\n",
      "pennystocks\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 230096\n",
      "Number of new IDs that were filtered: 34\n",
      "-------------------\n",
      "pennystocks\n",
      "formality\n",
      "Number of new IDs that passed filter added: 229927\n",
      "Number of new IDs that were filtered: 78\n",
      "-------------------\n",
      "asksciencediscussion\n",
      "formality\n",
      "Number of new IDs that passed filter added: 211879\n",
      "Number of new IDs that were filtered: 71\n",
      "-------------------\n",
      "stocks\n",
      "length\n",
      "Number of new IDs that passed filter added: 220571\n",
      "Number of new IDs that were filtered: 574\n",
      "-------------------\n",
      "askscience\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 226288\n",
      "Number of new IDs that were filtered: 17\n",
      "-------------------\n",
      "askmen\n",
      "length\n",
      "Number of new IDs that passed filter added: 243709\n",
      "Number of new IDs that were filtered: 1376\n",
      "-------------------\n",
      "askscience\n",
      "formality\n",
      "Number of new IDs that passed filter added: 226183\n",
      "Number of new IDs that were filtered: 57\n",
      "-------------------\n",
      "wallstreetbets\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 223608\n",
      "Number of new IDs that were filtered: 47\n",
      "-------------------\n",
      "wallstreetbets\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 223601\n",
      "Number of new IDs that were filtered: 69\n",
      "-------------------\n",
      "asktransgender\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 195724\n",
      "Number of new IDs that were filtered: 141\n",
      "-------------------\n",
      "republican\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 221056\n",
      "Number of new IDs that were filtered: 59\n",
      "-------------------\n",
      "pennystocks\n",
      "length\n",
      "Number of new IDs that passed filter added: 229479\n",
      "Number of new IDs that were filtered: 1166\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "humor\n",
      "Number of new IDs that passed filter added: 230392\n",
      "Number of new IDs that were filtered: 123\n",
      "-------------------\n",
      "democrats\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 200524\n",
      "Number of new IDs that were filtered: 36\n",
      "-------------------\n",
      "shittyaskscience\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 197647\n",
      "Number of new IDs that were filtered: 53\n",
      "-------------------\n",
      "republican\n",
      "length\n",
      "Number of new IDs that passed filter added: 220841\n",
      "Number of new IDs that were filtered: 509\n",
      "-------------------\n",
      "askscience\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 226278\n",
      "Number of new IDs that were filtered: 47\n",
      "-------------------\n",
      "stocks\n",
      "formality\n",
      "Number of new IDs that passed filter added: 219410\n",
      "Number of new IDs that were filtered: 50\n",
      "-------------------\n",
      "askscience\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 226358\n",
      "Number of new IDs that were filtered: 27\n",
      "-------------------\n",
      "pennystocks\n",
      "humor\n",
      "Number of new IDs that passed filter added: 230172\n",
      "Number of new IDs that were filtered: 38\n",
      "-------------------\n",
      "wallstreetbetsnew\n",
      "formality\n",
      "Number of new IDs that passed filter added: 230324\n",
      "Number of new IDs that were filtered: 186\n",
      "-------------------\n",
      "shittyaskscience\n",
      "length\n",
      "Number of new IDs that passed filter added: 197023\n",
      "Number of new IDs that were filtered: 767\n",
      "-------------------\n",
      "asktransgender\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 195774\n",
      "Number of new IDs that were filtered: 46\n",
      "-------------------\n",
      "stocks\n",
      "politeness\n",
      "Number of new IDs that passed filter added: 219719\n",
      "Number of new IDs that were filtered: 26\n",
      "-------------------\n",
      "askmen\n",
      "supportiveness\n",
      "Number of new IDs that passed filter added: 245700\n",
      "Number of new IDs that were filtered: 95\n",
      "-------------------\n",
      "republican\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 220666\n",
      "Number of new IDs that were filtered: 94\n",
      "-------------------\n",
      "askwomen\n",
      "sarcasm\n",
      "Number of new IDs that passed filter added: 244193\n",
      "Number of new IDs that were filtered: 42\n",
      "-------------------\n",
      "Total number of synthetic comments considered: 17156690\n",
      "Total number of synthetic comments filtered: 15192\n",
      "% of the synthetic comments filtered: 0.0008854854870024463\n",
      "Total number of synthetic comments left: 17141498\n",
      "total added: 17141498\n",
      "total filtered: 15192\n"
     ]
    }
   ],
   "source": [
    "apply_lexical_filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3bdf6-c1cb-431e-b442-a31bce383259",
   "metadata": {},
   "source": [
    "### Fluency Filter (PPL)\n",
    "- Employing DialoGPT, a model finetuned on 140M Reddit conversations, to compute perplexity of synthetic comments and filter them accordingly\n",
    "- TO ensure that the synthetic comments are as fluent as the original, human-written ones, we exclude synthetic comments with perplexity values outside the range of +-1 standard deviation (6860 PPL) from the mean perplexity (2747 PPL) of the original comments. We filter everything else.\n",
    "- At this stage, we employed SLURM and sent GPU jobs to compute the perplexity of the original and synthetic comments.\n",
    "- The GPU sbatch script will call on 'perplexity_compute.py' which is provided within valueScope/style_transfer/filter/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1095790a-809c-4af5-a558-c66fc955c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in prior style transfer generations to avoid computing the perplexity for new generations\n",
    "perplexity_computed = read_existing_dictionary(PERPLEXITY_FORMATTED_DIR)\n",
    "reddit_to_lexical_data = read_existing_dictionary_key_value(LINGUISTIC_FILTER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7255f72-a1b3-4d53-b257-58e4f8917c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to automate bash scripting\n",
    "def write_script(subreddit, dimension):\n",
    "    script = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=perplexity_sbatch_{SUBREDDIT}_{DIMENSION}\n",
    "#SBATCH --mail-type=FAIL,INVALID_DEPEND\n",
    "#SBATCH --mail-user=\n",
    "#SBATCH --account=\n",
    "#SBATCH --partition=ckpt\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --cpus-per-task=5\n",
    "#SBATCH --mem=100G\n",
    "#SBATCH --gpus=1\n",
    "#SBATCH --time=2-00:00:00 # Max runt:ime in DD-HH:MM:SS format.\n",
    "#SBATCH --chdir=/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/\n",
    "#SBATCH --export=all\n",
    "#SBATCH --output=/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/logs/%x-%j.out # where STDOUT goes\n",
    "#SBATCH --error=/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/logs/%x-%j.err # where STDERR goes\n",
    "\n",
    "# Your programs to run.\n",
    "source /mmfs1/home/hjung10/.bashrc\n",
    "conda activate jupyter-notebook\n",
    "cd /gscratch/argon/hjung10\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING=1 python /gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/scripts/perplexity_compute.py --subreddit '{SUBREDDIT}' --dimension '{DIMENSION}'\"\"\"\n",
    "\n",
    "    script = script.replace('{SUBREDDIT}', subreddit)\n",
    "    script = script.replace('{DIMENSION}', dimension)\n",
    "    return script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "151bebcd-81ba-4293-92f5-183bc3a99d28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first step is to compute perplexity values for any new style transfer generations\n",
    "def determine_new_style_transfers_needed(reddit_to_lexical_data, perplexity_computed):\n",
    "    reddit_to_compute_needed = dict()\n",
    "    \n",
    "    for reddit_json, dict_synthetic in reddit_to_lexical_data.items():\n",
    "        # contains set of synthetic comment IDs whose perplexity have already been computed\n",
    "        comments_computed_set = None\n",
    "        if reddit_json in perplexity_computed:\n",
    "            comments_computed_set = perplexity_computed[reddit_json]\n",
    "    \n",
    "        # dictionary to keep track of comments that we need to compute perplexity for\n",
    "        dict_compute = dict() \n",
    "        int_computed = 0\n",
    "        for synthetic_id, tuple_comments in dict_synthetic.items():\n",
    "    \n",
    "            # the perplexity for the synthetic comment has not been computed; add to compute list\n",
    "            if comments_computed_set == None or synthetic_id not in comments_computed_set:\n",
    "                dict_compute[synthetic_id] = tuple_comments\n",
    "            else:\n",
    "                int_computed += 1\n",
    "        print(reddit_json)\n",
    "        print(\"Number of synthetic comments needed for compute: \" + str(len(dict_compute)))\n",
    "        print(\"Number of synthetic comments already computed: \" + str(int_computed))\n",
    "        reddit_to_compute_needed [reddit_json] = dict_compute\n",
    "        print()\n",
    "    return reddit_to_compute_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7bb9982-5ffe-4092-8f53-11c519d213ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(reddit_to_compute_needed):\n",
    "    for file_name, content in reddit_to_compute_needed.items():\n",
    "        # only executing batch script if new perplexity needs to be computed\n",
    "        if len(content) > 0:\n",
    "            reddit = file_name.split('_')[0]\n",
    "            dimension = file_name.split('_')[1].split('.')[0]\n",
    "            \n",
    "            script = write_script(reddit, dimension)\n",
    "            sbatch_file = 'run_perplexity.sbatch'\n",
    "            with open(SCRIPTS_DIR + sbatch_file, 'w') as file:\n",
    "                file.write(script)\n",
    "    \n",
    "            print(reddit)\n",
    "            print(dimension)\n",
    "            execute_string = SCRIPTS_DIR + sbatch_file\n",
    "            subprocess.run(['sbatch', execute_string])\n",
    "            print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc7c7123-c2fa-4596-afb4-4e3d54675d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 245597\n",
      "\n",
      "libertarian_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 212307\n",
      "\n",
      "askscience_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 28689\n",
      "Number of synthetic comments already computed: 197746\n",
      "\n",
      "askwomen_length.jsonl\n",
      "Number of synthetic comments needed for compute: 35428\n",
      "Number of synthetic comments already computed: 207596\n",
      "\n",
      "askwomen_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 34730\n",
      "Number of synthetic comments already computed: 209480\n",
      "\n",
      "shittyaskscience_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 197662\n",
      "\n",
      "libertarian_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 212214\n",
      "\n",
      "askscience_length.jsonl\n",
      "Number of synthetic comments needed for compute: 11518\n",
      "Number of synthetic comments already computed: 214705\n",
      "\n",
      "asksciencediscussion_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 211885\n",
      "\n",
      "democrats_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 199791\n",
      "\n",
      "asktransgender_length.jsonl\n",
      "Number of synthetic comments needed for compute: 32191\n",
      "Number of synthetic comments already computed: 165029\n",
      "\n",
      "asksciencediscussion_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 211855\n",
      "\n",
      "stocks_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 219794\n",
      "\n",
      "republican_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 2885\n",
      "Number of synthetic comments already computed: 217949\n",
      "\n",
      "libertarian_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 212282\n",
      "\n",
      "asktransgender_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 195616\n",
      "\n",
      "shittyaskscience_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 197663\n",
      "\n",
      "wallstreetbets_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 223703\n",
      "\n",
      "stocks_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 219662\n",
      "\n",
      "askmen_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 33090\n",
      "Number of synthetic comments already computed: 212418\n",
      "\n",
      "pennystocks_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 230162\n",
      "\n",
      "democrats_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 10873\n",
      "Number of synthetic comments already computed: 189278\n",
      "\n",
      "wallstreetbetsnew_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 230403\n",
      "\n",
      "stocks_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 219509\n",
      "\n",
      "asksciencediscussion_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 211975\n",
      "\n",
      "asksciencediscussion_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 211954\n",
      "\n",
      "askmen_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 245527\n",
      "\n",
      "askwomen_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 5659\n",
      "Number of synthetic comments already computed: 238518\n",
      "\n",
      "wallstreetbetsnew_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 230497\n",
      "\n",
      "askwomen_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 2115\n",
      "Number of synthetic comments already computed: 242112\n",
      "\n",
      "asksciencediscussion_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 13389\n",
      "Number of synthetic comments already computed: 198568\n",
      "\n",
      "libertarian_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 212306\n",
      "\n",
      "shittyaskscience_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 197680\n",
      "\n",
      "shittyaskscience_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 197674\n",
      "\n",
      "democrats_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 200712\n",
      "\n",
      "asktransgender_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 19385\n",
      "Number of synthetic comments already computed: 176601\n",
      "\n",
      "democrats_length.jsonl\n",
      "Number of synthetic comments needed for compute: 30776\n",
      "Number of synthetic comments already computed: 168813\n",
      "\n",
      "libertarian_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 210001\n",
      "\n",
      "republican_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 221389\n",
      "\n",
      "asktransgender_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 195657\n",
      "\n",
      "wallstreetbets_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 12886\n",
      "Number of synthetic comments already computed: 210788\n",
      "\n",
      "wallstreetbetsnew_length.jsonl\n",
      "Number of synthetic comments needed for compute: 10878\n",
      "Number of synthetic comments already computed: 217138\n",
      "\n",
      "wallstreetbets_length.jsonl\n",
      "Number of synthetic comments needed for compute: 6947\n",
      "Number of synthetic comments already computed: 215576\n",
      "\n",
      "pennystocks_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 230418\n",
      "\n",
      "wallstreetbetsnew_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 230338\n",
      "\n",
      "libertarian_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 210280\n",
      "\n",
      "wallstreetbets_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 223586\n",
      "\n",
      "askmen_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 41271\n",
      "Number of synthetic comments already computed: 203992\n",
      "\n",
      "republican_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 220716\n",
      "\n",
      "democrats_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 200313\n",
      "\n",
      "askwomen_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 47130\n",
      "Number of synthetic comments already computed: 197081\n",
      "\n",
      "pennystocks_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 230096\n",
      "\n",
      "pennystocks_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 229927\n",
      "\n",
      "asksciencediscussion_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 12555\n",
      "Number of synthetic comments already computed: 199324\n",
      "\n",
      "stocks_length.jsonl\n",
      "Number of synthetic comments needed for compute: 2725\n",
      "Number of synthetic comments already computed: 217846\n",
      "\n",
      "askscience_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 9980\n",
      "Number of synthetic comments already computed: 216308\n",
      "\n",
      "askmen_length.jsonl\n",
      "Number of synthetic comments needed for compute: 39560\n",
      "Number of synthetic comments already computed: 204149\n",
      "\n",
      "askscience_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 226183\n",
      "\n",
      "wallstreetbets_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 223608\n",
      "\n",
      "wallstreetbets_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 223601\n",
      "\n",
      "asktransgender_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 10910\n",
      "Number of synthetic comments already computed: 184814\n",
      "\n",
      "republican_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 221056\n",
      "\n",
      "pennystocks_length.jsonl\n",
      "Number of synthetic comments needed for compute: 20549\n",
      "Number of synthetic comments already computed: 208930\n",
      "\n",
      "wallstreetbetsnew_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 230392\n",
      "\n",
      "democrats_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 200524\n",
      "\n",
      "shittyaskscience_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 197647\n",
      "\n",
      "republican_length.jsonl\n",
      "Number of synthetic comments needed for compute: 17887\n",
      "Number of synthetic comments already computed: 202954\n",
      "\n",
      "askscience_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 32697\n",
      "Number of synthetic comments already computed: 193581\n",
      "\n",
      "stocks_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 219410\n",
      "\n",
      "askscience_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 23030\n",
      "Number of synthetic comments already computed: 203328\n",
      "\n",
      "pennystocks_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 230172\n",
      "\n",
      "wallstreetbetsnew_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 230324\n",
      "\n",
      "shittyaskscience_length.jsonl\n",
      "Number of synthetic comments needed for compute: 13131\n",
      "Number of synthetic comments already computed: 183892\n",
      "\n",
      "asktransgender_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 195774\n",
      "\n",
      "stocks_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 219719\n",
      "\n",
      "askmen_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 245700\n",
      "\n",
      "republican_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 220666\n",
      "\n",
      "askwomen_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 14448\n",
      "Number of synthetic comments already computed: 229745\n",
      "\n",
      "askscience\n",
      "humor\n",
      "Submitted batch job 18917857\n",
      "----\n",
      "askwomen\n",
      "length\n",
      "Submitted batch job 18917858\n",
      "----\n",
      "askwomen\n",
      "supportiveness\n",
      "Submitted batch job 18917859\n",
      "----\n",
      "askscience\n",
      "length\n",
      "Submitted batch job 18917860\n",
      "----\n",
      "asktransgender\n",
      "length\n",
      "Submitted batch job 18917861\n",
      "----\n",
      "republican\n",
      "politeness\n",
      "Submitted batch job 18917862\n",
      "----\n",
      "askmen\n",
      "humor\n",
      "Submitted batch job 18917863\n",
      "----\n",
      "democrats\n",
      "supportiveness\n",
      "Submitted batch job 18917864\n",
      "----\n",
      "askwomen\n",
      "formality\n",
      "Submitted batch job 18917865\n",
      "----\n",
      "askwomen\n",
      "politeness\n",
      "Submitted batch job 18917866\n",
      "----\n",
      "asksciencediscussion\n",
      "sarcasm\n",
      "Submitted batch job 18917867\n",
      "----\n",
      "asktransgender\n",
      "humor\n",
      "Submitted batch job 18917868\n",
      "----\n",
      "democrats\n",
      "length\n",
      "Submitted batch job 18917869\n",
      "----\n",
      "wallstreetbets\n",
      "formality\n",
      "Submitted batch job 18917870\n",
      "----\n",
      "wallstreetbetsnew\n",
      "length\n",
      "Submitted batch job 18917871\n",
      "----\n",
      "wallstreetbets\n",
      "length\n",
      "Submitted batch job 18917872\n",
      "----\n",
      "askmen\n",
      "politeness\n",
      "Submitted batch job 18917873\n",
      "----\n",
      "askwomen\n",
      "humor\n",
      "Submitted batch job 18917874\n",
      "----\n",
      "asksciencediscussion\n",
      "formality\n",
      "Submitted batch job 18917875\n",
      "----\n",
      "stocks\n",
      "length\n",
      "Submitted batch job 18917876\n",
      "----\n",
      "askscience\n",
      "supportiveness\n",
      "Submitted batch job 18917877\n",
      "----\n",
      "askmen\n",
      "length\n",
      "Submitted batch job 18917878\n",
      "----\n",
      "asktransgender\n",
      "sarcasm\n",
      "Submitted batch job 18917879\n",
      "----\n",
      "pennystocks\n",
      "length\n",
      "Submitted batch job 18917880\n",
      "----\n",
      "republican\n",
      "length\n",
      "Submitted batch job 18917881\n",
      "----\n",
      "askscience\n",
      "politeness\n",
      "Submitted batch job 18917882\n",
      "----\n",
      "askscience\n",
      "sarcasm\n",
      "Submitted batch job 18917883\n",
      "----\n",
      "shittyaskscience\n",
      "length\n",
      "Submitted batch job 18917884\n",
      "----\n",
      "askwomen\n",
      "sarcasm\n",
      "Submitted batch job 18917885\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# determining which subreddit-dimension has new style transferred comments that needs to perplexity computation\n",
    "reddit_to_compute_needed = determine_new_style_transfers_needed(reddit_to_lexical_data, perplexity_computed)\n",
    "compute_perplexity(reddit_to_compute_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74e1a364-d943-491e-bc43-d02c538eff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fluency_filter():\n",
    "    # rereading in new perplexity computations so we do not add duplicate comments in the file\n",
    "    reddit_to_ppl_data = read_existing_dictionary(FLUENCY_FILTER_DIR)\n",
    "    reddit_to_ppl_filtered = read_existing_dictionary(FLUENCY_FILTERED_DIR)\n",
    "    \n",
    "    # containing the perplexity numbers so we can easily check\n",
    "    perplexity_computed = read_existing_dictionary_key_value(PERPLEXITY_FORMATTED_DIR)\n",
    "    \n",
    "    # data from the prior filter (continuing where we left off)\n",
    "    reddit_to_lexical_data = read_existing_dictionary_key_value(LINGUISTIC_FILTER_DIR)\n",
    "\n",
    "    # sanity check\n",
    "    total_added = 0\n",
    "    total_filtered = 0\n",
    "    \n",
    "    total_synthetic_considered = 0\n",
    "    total_synthetic_filtered = 0\n",
    "    total_synthetic_left = 0\n",
    "    for reddit_json, dict_synthetic in reddit_to_lexical_data.items():\n",
    "        print(reddit_json)\n",
    "    \n",
    "        list_dict = []\n",
    "        list_filtered = []\n",
    "        for synthetic_id, list_comments in dict_synthetic.items():\n",
    "            total_synthetic_considered += 1\n",
    "    \n",
    "            if reddit_json in perplexity_computed:\n",
    "                perplexity = 0\n",
    "                if synthetic_id in perplexity_computed[reddit_json]:\n",
    "                    perplexity = perplexity_computed[reddit_json][synthetic_id]\n",
    "                else:\n",
    "                    print(list_comments)\n",
    "    \n",
    "                # check perplexity within the threshold\n",
    "                id_to_comments = dict()\n",
    "\n",
    "                if perplexity >= (2747-6860) and perplexity <= (2747+6860):\n",
    "                    total_synthetic_left +=1\n",
    "                    id_to_comments[synthetic_id] = list_comments\n",
    "                    list_dict.append(id_to_comments)\n",
    "                else:\n",
    "                    total_synthetic_filtered += 1\n",
    "                    id_to_comments[synthetic_id] = list_comments\n",
    "                    list_filtered.append(id_to_comments)\n",
    "         # saving to the file\n",
    "        added = save_jsonl(FLUENCY_FILTER_DIR, reddit_json, list_dict, reddit_to_ppl_data)\n",
    "    \n",
    "        # saving filtered data\n",
    "        added_filtered = save_jsonl(FLUENCY_FILTERED_DIR, reddit_json, list_filtered, reddit_to_ppl_filtered)\n",
    "        print(\"Number of new IDs that passed filter added: \" + str(added))\n",
    "        print(\"Number of new IDs that were filtered: \" + str(added_filtered))\n",
    "        print(\"-------------------\")\n",
    "        \n",
    "        total_added += added\n",
    "        total_filtered += added_filtered\n",
    "    \n",
    "    print(\"Total number of synthetic comments considered: \" + str(total_synthetic_considered))\n",
    "    print(\"Total number of synthetic comments filtered: \" + str(total_synthetic_filtered))\n",
    "    print(\"% of the synthetic comments filtered: \" + str((total_synthetic_filtered) / total_synthetic_considered))\n",
    "    print(\"Total number of synthetic comments left: \" + str(total_synthetic_left))\n",
    "\n",
    "    \n",
    "    print(\"total added: \" + str(total_added))\n",
    "    print(\"total filtered: \" + str(total_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae0bd931-63d6-4685-afcf-a1a9d9fefdba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen_formality.jsonl\n",
      "Number of new IDs that passed filter added: 244405\n",
      "Number of new IDs that were filtered: 1192\n",
      "-------------------\n",
      "libertarian_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 212247\n",
      "Number of new IDs that were filtered: 60\n",
      "-------------------\n",
      "askscience_humor.jsonl\n",
      "Number of new IDs that passed filter added: 226335\n",
      "Number of new IDs that were filtered: 100\n",
      "-------------------\n",
      "askwomen_length.jsonl\n",
      "Number of new IDs that passed filter added: 230621\n",
      "Number of new IDs that were filtered: 12403\n",
      "-------------------\n",
      "askwomen_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 244065\n",
      "Number of new IDs that were filtered: 145\n",
      "-------------------\n",
      "shittyaskscience_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 197217\n",
      "Number of new IDs that were filtered: 445\n",
      "-------------------\n",
      "libertarian_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 212107\n",
      "Number of new IDs that were filtered: 107\n",
      "-------------------\n",
      "askscience_length.jsonl\n",
      "Number of new IDs that passed filter added: 217867\n",
      "Number of new IDs that were filtered: 8356\n",
      "-------------------\n",
      "asksciencediscussion_length.jsonl\n",
      "Number of new IDs that passed filter added: 203501\n",
      "Number of new IDs that were filtered: 8384\n",
      "-------------------\n",
      "democrats_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 199435\n",
      "Number of new IDs that were filtered: 356\n",
      "-------------------\n",
      "asktransgender_length.jsonl\n",
      "Number of new IDs that passed filter added: 189625\n",
      "Number of new IDs that were filtered: 7595\n",
      "-------------------\n",
      "asksciencediscussion_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 211764\n",
      "Number of new IDs that were filtered: 91\n",
      "-------------------\n",
      "stocks_humor.jsonl\n",
      "Number of new IDs that passed filter added: 219638\n",
      "Number of new IDs that were filtered: 156\n",
      "-------------------\n",
      "republican_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 220730\n",
      "Number of new IDs that were filtered: 104\n",
      "-------------------\n",
      "libertarian_length.jsonl\n",
      "Number of new IDs that passed filter added: 202810\n",
      "Number of new IDs that were filtered: 9472\n",
      "-------------------\n",
      "asktransgender_formality.jsonl\n",
      "Number of new IDs that passed filter added: 194921\n",
      "Number of new IDs that were filtered: 695\n",
      "-------------------\n",
      "shittyaskscience_humor.jsonl\n",
      "Number of new IDs that passed filter added: 197498\n",
      "Number of new IDs that were filtered: 165\n",
      "-------------------\n",
      "wallstreetbets_humor.jsonl\n",
      "Number of new IDs that passed filter added: 223176\n",
      "Number of new IDs that were filtered: 527\n",
      "-------------------\n",
      "stocks_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 219327\n",
      "Number of new IDs that were filtered: 335\n",
      "-------------------\n",
      "askmen_humor.jsonl\n",
      "Number of new IDs that passed filter added: 245296\n",
      "Number of new IDs that were filtered: 212\n",
      "-------------------\n",
      "pennystocks_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 229977\n",
      "Number of new IDs that were filtered: 185\n",
      "-------------------\n",
      "democrats_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 199932\n",
      "Number of new IDs that were filtered: 219\n",
      "-------------------\n",
      "wallstreetbetsnew_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 230132\n",
      "Number of new IDs that were filtered: 271\n",
      "-------------------\n",
      "stocks_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 219348\n",
      "Number of new IDs that were filtered: 161\n",
      "-------------------\n",
      "asksciencediscussion_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 211837\n",
      "Number of new IDs that were filtered: 138\n",
      "-------------------\n",
      "asksciencediscussion_humor.jsonl\n",
      "Number of new IDs that passed filter added: 211879\n",
      "Number of new IDs that were filtered: 75\n",
      "-------------------\n",
      "askmen_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 244973\n",
      "Number of new IDs that were filtered: 554\n",
      "-------------------\n",
      "askwomen_formality.jsonl\n",
      "Number of new IDs that passed filter added: 243355\n",
      "Number of new IDs that were filtered: 822\n",
      "-------------------\n",
      "wallstreetbetsnew_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 229736\n",
      "Number of new IDs that were filtered: 761\n",
      "-------------------\n",
      "askwomen_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 244160\n",
      "Number of new IDs that were filtered: 67\n",
      "-------------------\n",
      "asksciencediscussion_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 211764\n",
      "Number of new IDs that were filtered: 193\n",
      "-------------------\n",
      "libertarian_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 212115\n",
      "Number of new IDs that were filtered: 191\n",
      "-------------------\n",
      "shittyaskscience_formality.jsonl\n",
      "Number of new IDs that passed filter added: 196666\n",
      "Number of new IDs that were filtered: 1014\n",
      "-------------------\n",
      "shittyaskscience_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 197611\n",
      "Number of new IDs that were filtered: 63\n",
      "-------------------\n",
      "democrats_humor.jsonl\n",
      "Number of new IDs that passed filter added: 200547\n",
      "Number of new IDs that were filtered: 165\n",
      "-------------------\n",
      "asktransgender_humor.jsonl\n",
      "Number of new IDs that passed filter added: 195871\n",
      "Number of new IDs that were filtered: 115\n",
      "-------------------\n",
      "democrats_length.jsonl\n",
      "Number of new IDs that passed filter added: 189011\n",
      "Number of new IDs that were filtered: 10578\n",
      "-------------------\n",
      "libertarian_humor.jsonl\n",
      "Number of new IDs that passed filter added: 209907\n",
      "Number of new IDs that were filtered: 94\n",
      "-------------------\n",
      "republican_formality.jsonl\n",
      "Number of new IDs that passed filter added: 220487\n",
      "Number of new IDs that were filtered: 902\n",
      "-------------------\n",
      "asktransgender_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 195489\n",
      "Number of new IDs that were filtered: 168\n",
      "-------------------\n",
      "wallstreetbets_formality.jsonl\n",
      "Number of new IDs that passed filter added: 220808\n",
      "Number of new IDs that were filtered: 2866\n",
      "-------------------\n",
      "wallstreetbetsnew_length.jsonl\n",
      "Number of new IDs that passed filter added: 209175\n",
      "Number of new IDs that were filtered: 18841\n",
      "-------------------\n",
      "wallstreetbets_length.jsonl\n",
      "Number of new IDs that passed filter added: 205201\n",
      "Number of new IDs that were filtered: 17322\n",
      "-------------------\n",
      "pennystocks_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 229950\n",
      "Number of new IDs that were filtered: 468\n",
      "-------------------\n",
      "wallstreetbetsnew_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 228870\n",
      "Number of new IDs that were filtered: 1468\n",
      "-------------------\n",
      "libertarian_formality.jsonl\n",
      "Number of new IDs that passed filter added: 209674\n",
      "Number of new IDs that were filtered: 606\n",
      "-------------------\n",
      "wallstreetbets_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 223392\n",
      "Number of new IDs that were filtered: 194\n",
      "-------------------\n",
      "askmen_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 245164\n",
      "Number of new IDs that were filtered: 99\n",
      "-------------------\n",
      "republican_humor.jsonl\n",
      "Number of new IDs that passed filter added: 220543\n",
      "Number of new IDs that were filtered: 173\n",
      "-------------------\n",
      "democrats_formality.jsonl\n",
      "Number of new IDs that passed filter added: 199367\n",
      "Number of new IDs that were filtered: 946\n",
      "-------------------\n",
      "askwomen_humor.jsonl\n",
      "Number of new IDs that passed filter added: 244080\n",
      "Number of new IDs that were filtered: 131\n",
      "-------------------\n",
      "pennystocks_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 229233\n",
      "Number of new IDs that were filtered: 863\n",
      "-------------------\n",
      "pennystocks_formality.jsonl\n",
      "Number of new IDs that passed filter added: 227650\n",
      "Number of new IDs that were filtered: 2277\n",
      "-------------------\n",
      "asksciencediscussion_formality.jsonl\n",
      "Number of new IDs that passed filter added: 211300\n",
      "Number of new IDs that were filtered: 579\n",
      "-------------------\n",
      "stocks_length.jsonl\n",
      "Number of new IDs that passed filter added: 208264\n",
      "Number of new IDs that were filtered: 12307\n",
      "-------------------\n",
      "askscience_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 226172\n",
      "Number of new IDs that were filtered: 116\n",
      "-------------------\n",
      "askmen_length.jsonl\n",
      "Number of new IDs that passed filter added: 230915\n",
      "Number of new IDs that were filtered: 12794\n",
      "-------------------\n",
      "askscience_formality.jsonl\n",
      "Number of new IDs that passed filter added: 225664\n",
      "Number of new IDs that were filtered: 519\n",
      "-------------------\n",
      "wallstreetbets_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 222955\n",
      "Number of new IDs that were filtered: 653\n",
      "-------------------\n",
      "wallstreetbets_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 222408\n",
      "Number of new IDs that were filtered: 1193\n",
      "-------------------\n",
      "asktransgender_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 195501\n",
      "Number of new IDs that were filtered: 223\n",
      "-------------------\n",
      "republican_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 220859\n",
      "Number of new IDs that were filtered: 197\n",
      "-------------------\n",
      "pennystocks_length.jsonl\n",
      "Number of new IDs that passed filter added: 213909\n",
      "Number of new IDs that were filtered: 15570\n",
      "-------------------\n",
      "wallstreetbetsnew_humor.jsonl\n",
      "Number of new IDs that passed filter added: 229852\n",
      "Number of new IDs that were filtered: 540\n",
      "-------------------\n",
      "democrats_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 200438\n",
      "Number of new IDs that were filtered: 86\n",
      "-------------------\n",
      "shittyaskscience_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 197353\n",
      "Number of new IDs that were filtered: 294\n",
      "-------------------\n",
      "republican_length.jsonl\n",
      "Number of new IDs that passed filter added: 209553\n",
      "Number of new IDs that were filtered: 11288\n",
      "-------------------\n",
      "askscience_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 226183\n",
      "Number of new IDs that were filtered: 95\n",
      "-------------------\n",
      "stocks_formality.jsonl\n",
      "Number of new IDs that passed filter added: 218210\n",
      "Number of new IDs that were filtered: 1200\n",
      "-------------------\n",
      "askscience_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 226200\n",
      "Number of new IDs that were filtered: 158\n",
      "-------------------\n",
      "pennystocks_humor.jsonl\n",
      "Number of new IDs that passed filter added: 229834\n",
      "Number of new IDs that were filtered: 338\n",
      "-------------------\n",
      "wallstreetbetsnew_formality.jsonl\n",
      "Number of new IDs that passed filter added: 227108\n",
      "Number of new IDs that were filtered: 3216\n",
      "-------------------\n",
      "shittyaskscience_length.jsonl\n",
      "Number of new IDs that passed filter added: 183729\n",
      "Number of new IDs that were filtered: 13294\n",
      "-------------------\n",
      "asktransgender_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 195721\n",
      "Number of new IDs that were filtered: 53\n",
      "-------------------\n",
      "stocks_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 219627\n",
      "Number of new IDs that were filtered: 92\n",
      "-------------------\n",
      "askmen_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 245433\n",
      "Number of new IDs that were filtered: 267\n",
      "-------------------\n",
      "republican_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 220287\n",
      "Number of new IDs that were filtered: 379\n",
      "-------------------\n",
      "askwomen_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 243893\n",
      "Number of new IDs that were filtered: 300\n",
      "-------------------\n",
      "Total number of synthetic comments considered: 17141498\n",
      "Total number of synthetic comments filtered: 189641\n",
      "% of the synthetic comments filtered: 0.011063268799494654\n",
      "Total number of synthetic comments left: 16951857\n",
      "total added: 16951857\n",
      "total filtered: 189641\n"
     ]
    }
   ],
   "source": [
    "# only execute this cell once the perplexity scores have been all computed (e.g. GPU jobs are done)\n",
    "apply_fluency_filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c96ac-ad69-47f5-b303-ffd0e7c21205",
   "metadata": {},
   "source": [
    "### Content Preservation Filter\n",
    "- Here, we employ BERTScore, which has been shown to align with human judgments in prior works. After qualitative investigation of various BERTScore values and the content preservations between original vs. synthetic comments, we decided to use BERTScore=0.5 as a threshold, removing synthetic comments whose BERTScore values fall below 0.5. \n",
    "- BERTScore computed using the model \"microsoft/deberta-xlarge-mnli\" which best aligns with human judgement according to the authors (https://github.com/Tiiiger/bert_score#readme)\n",
    "- @inproceedings{bert-score,\n",
    "  title={BERTScore: Evaluating Text Generation with BERT},\n",
    "  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},\n",
    "  booktitle={International Conference on Learning Representations},\n",
    "  year={2020},\n",
    "  url={https://openreview.net/forum?id=SkeHuCVFDr}}\n",
    "- At this stage, we also use SLURM and send GPU jobs to compute the bert scores. The GPU sbatch script will call on 'bert-score-compute.py' which is provided within valueScope/style_transfer/filter/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fb2b459-5cee-4ac0-8781-d20c0c41651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in prior style transfer generations to avoid computing the perplexity for new generations\n",
    "bertscore_computed = read_existing_dictionary(BERTSCORE_FORMATTED_DIR)\n",
    "reddit_to_ppl_data = read_existing_dictionary_key_value(FLUENCY_FILTER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6453b52-5f95-4427-9862-374d69443236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to automate bash scripting\n",
    "def write_script_bertscore(subreddit, dimension):\n",
    "    script = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=bertscore_sbatch_{SUBREDDIT}_{DIMENSION}\n",
    "#SBATCH --mail-type=FAIL,INVALID_DEPEND\n",
    "#SBATCH --mail-user=\n",
    "#SBATCH --account=\n",
    "#SBATCH --partition=ckpt\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --cpus-per-task=5\n",
    "#SBATCH --mem=100G\n",
    "#SBATCH --gpus=1\n",
    "#SBATCH --time=2-00:00:00 # Max runt:ime in DD-HH:MM:SS format.\n",
    "#SBATCH --chdir=/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/\n",
    "#SBATCH --export=all\n",
    "#SBATCH --output=/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/logs/%x-%j.out # where STDOUT goes\n",
    "#SBATCH --error=/gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/logs/%x-%j.err # where STDERR goes\n",
    "\n",
    "# Your programs to run.\n",
    "source /mmfs1/home/hjung10/.bashrc\n",
    "export HF_HOME=/gscratch/scrubbed/hjung10/{SUBREDDIT}-{DIMENSION}\n",
    "conda activate jupyter-notebook\n",
    "cd /gscratch/argon/hjung10\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING=1 python /gscratch/argon/hjung10/norm_discovery_project/code/style_transfer_filter/scripts/bert-score-compute.py --subreddit '{SUBREDDIT}' --dimension '{DIMENSION}'\"\"\"\n",
    "\n",
    "    script = script.replace('{SUBREDDIT}', subreddit)\n",
    "    script = script.replace('{DIMENSION}', dimension)\n",
    "    return script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36add0e4-dded-42f3-a136-b20c72e4454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine new bertscore computations needed\n",
    "def determine_new_bertscores_needed(reddit_to_ppl_data, bertscore_computed):\n",
    "    reddit_to_compute_needed = dict()\n",
    "\n",
    "    for reddit_json, dict_synthetic in reddit_to_ppl_data.items():\n",
    "        # contains set of synthetic comment IDs whose perplexity have already been computed\n",
    "        comments_computed_set = None\n",
    "        if reddit_json in bertscore_computed:\n",
    "            comments_computed_set = bertscore_computed[reddit_json]\n",
    "\n",
    "        # dictionary to keep track of comments that we need to compute perplexity for\n",
    "        dict_compute = dict() \n",
    "        int_computed = 0\n",
    "        for synthetic_id, tuple_comments in dict_synthetic.items():\n",
    "            if comments_computed_set == None or synthetic_id not in comments_computed_set:\n",
    "                dict_compute[synthetic_id] = tuple_comments\n",
    "            else:\n",
    "                int_computed += 1\n",
    "        print(reddit_json)\n",
    "        print(\"Number of synthetic comments needed for compute: \" + str(len(dict_compute)))\n",
    "        print(\"Number of synthetic comments already computed: \" + str(int_computed))\n",
    "        reddit_to_compute_needed [reddit_json] = dict_compute\n",
    "        print()\n",
    "    return reddit_to_compute_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24b49582-9a81-4a17-8c29-2d652f08df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(reddit_to_compute_needed):\n",
    "    for file_name, content in reddit_to_compute_needed.items():\n",
    "        # only executing batch script if new perplexity eneds to be computed\n",
    "        if len(content) > 0:\n",
    "            reddit = file_name.split('_')[0]\n",
    "            dimension = file_name.split('_')[1].split('.')[0]\n",
    "            \n",
    "            script = write_script_bertscore(reddit, dimension)\n",
    "            sbatch_file = 'run_bertscore.sbatch'\n",
    "            with open(SCRIPTS_DIR + sbatch_file, 'w') as file:\n",
    "                file.write(script)\n",
    "    \n",
    "            print(reddit)\n",
    "            print(dimension)\n",
    "            execute_string = SCRIPTS_DIR + sbatch_file\n",
    "            subprocess.run(['sbatch', execute_string])\n",
    "            print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "103f2619-c686-4e57-918b-ed72bf4e33da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 244405\n",
      "\n",
      "libertarian_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 212247\n",
      "\n",
      "askscience_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 226335\n",
      "\n",
      "askwomen_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 230621\n",
      "\n",
      "askwomen_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 244065\n",
      "\n",
      "shittyaskscience_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 197217\n",
      "\n",
      "libertarian_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 212107\n",
      "\n",
      "askscience_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 217867\n",
      "\n",
      "asksciencediscussion_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 203501\n",
      "\n",
      "democrats_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 199435\n",
      "\n",
      "asktransgender_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 189625\n",
      "\n",
      "asksciencediscussion_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 211764\n",
      "\n",
      "stocks_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 219638\n",
      "\n",
      "republican_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 220730\n",
      "\n",
      "libertarian_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 202810\n",
      "\n",
      "asktransgender_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 194921\n",
      "\n",
      "shittyaskscience_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 197498\n",
      "\n",
      "wallstreetbets_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 223176\n",
      "\n",
      "stocks_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 219327\n",
      "\n",
      "askmen_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 245296\n",
      "\n",
      "pennystocks_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 229977\n",
      "\n",
      "democrats_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 199932\n",
      "\n",
      "wallstreetbetsnew_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 20\n",
      "Number of synthetic comments already computed: 230112\n",
      "\n",
      "stocks_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 219348\n",
      "\n",
      "asksciencediscussion_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 211837\n",
      "\n",
      "asksciencediscussion_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 211879\n",
      "\n",
      "askmen_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 244973\n",
      "\n",
      "askwomen_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 243355\n",
      "\n",
      "wallstreetbetsnew_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 229736\n",
      "\n",
      "askwomen_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 244160\n",
      "\n",
      "asksciencediscussion_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 211764\n",
      "\n",
      "libertarian_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 212115\n",
      "\n",
      "shittyaskscience_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 196666\n",
      "\n",
      "shittyaskscience_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 197611\n",
      "\n",
      "democrats_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 200547\n",
      "\n",
      "asktransgender_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 195871\n",
      "\n",
      "democrats_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 189011\n",
      "\n",
      "libertarian_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 209907\n",
      "\n",
      "republican_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 220487\n",
      "\n",
      "asktransgender_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 195489\n",
      "\n",
      "wallstreetbets_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 220808\n",
      "\n",
      "wallstreetbetsnew_length.jsonl\n",
      "Number of synthetic comments needed for compute: 23\n",
      "Number of synthetic comments already computed: 209152\n",
      "\n",
      "wallstreetbets_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 205201\n",
      "\n",
      "pennystocks_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 229950\n",
      "\n",
      "wallstreetbetsnew_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 34\n",
      "Number of synthetic comments already computed: 228836\n",
      "\n",
      "libertarian_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 209674\n",
      "\n",
      "wallstreetbets_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 223392\n",
      "\n",
      "askmen_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 245164\n",
      "\n",
      "republican_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 220543\n",
      "\n",
      "democrats_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 199367\n",
      "\n",
      "askwomen_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 244080\n",
      "\n",
      "pennystocks_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 229233\n",
      "\n",
      "pennystocks_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 227650\n",
      "\n",
      "asksciencediscussion_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 211300\n",
      "\n",
      "stocks_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 208264\n",
      "\n",
      "askscience_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 226172\n",
      "\n",
      "askmen_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 230915\n",
      "\n",
      "askscience_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 225664\n",
      "\n",
      "wallstreetbets_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 222955\n",
      "\n",
      "wallstreetbets_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 222408\n",
      "\n",
      "asktransgender_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 195501\n",
      "\n",
      "republican_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 220859\n",
      "\n",
      "pennystocks_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 213909\n",
      "\n",
      "wallstreetbetsnew_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 229852\n",
      "\n",
      "democrats_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 200438\n",
      "\n",
      "shittyaskscience_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 197353\n",
      "\n",
      "republican_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 209553\n",
      "\n",
      "askscience_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 226183\n",
      "\n",
      "stocks_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 218210\n",
      "\n",
      "askscience_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 226200\n",
      "\n",
      "pennystocks_humor.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 229834\n",
      "\n",
      "wallstreetbetsnew_formality.jsonl\n",
      "Number of synthetic comments needed for compute: 30\n",
      "Number of synthetic comments already computed: 227078\n",
      "\n",
      "shittyaskscience_length.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 183729\n",
      "\n",
      "asktransgender_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 195721\n",
      "\n",
      "stocks_politeness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 219627\n",
      "\n",
      "askmen_supportiveness.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 245433\n",
      "\n",
      "republican_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 220287\n",
      "\n",
      "askwomen_sarcasm.jsonl\n",
      "Number of synthetic comments needed for compute: 0\n",
      "Number of synthetic comments already computed: 243893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determining which subreddit-dimension has new style transferred comments that needs to perplexity computation\n",
    "reddit_to_bertscore_compute_needed = determine_new_bertscores_needed(reddit_to_ppl_data, bertscore_computed)\n",
    "compute_bertscore(reddit_to_bertscore_compute_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35669f31-bd6a-4b39-b2b2-33e23d633c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_content_preservation_filter():\n",
    "    # rereading in new perplexity computations so we do not add duplicate comments in the file\n",
    "    reddit_to_bertscore_data = read_existing_dictionary(CONTENT_PRES_FILTER_DIR)\n",
    "    reddit_to_bertscore_filtered = read_existing_dictionary(CONTENT_PRES_FILTERED_DIR)\n",
    "    \n",
    "    # containing the perplexity numbers so we can easily check\n",
    "    bertscore_computed = read_existing_dictionary_key_value(BERTSCORE_FORMATTED_DIR)\n",
    "    \n",
    "    # data from the prior filter (continuing where we left off)\n",
    "    reddit_to_ppl_filter_data = read_existing_dictionary_key_value(FLUENCY_FILTER_DIR)\n",
    "\n",
    "    # sanity check\n",
    "    total_added = 0\n",
    "    total_filtered = 0\n",
    "    \n",
    "\n",
    "    total_synthetic_considered = 0\n",
    "    total_synthetic_filtered = 0\n",
    "    total_synthetic_left = 0\n",
    "    for reddit_json, dict_synthetic in reddit_to_ppl_filter_data.items():\n",
    "        print(reddit_json)\n",
    "    \n",
    "        list_dict = []\n",
    "        list_filtered = []\n",
    "        for synthetic_id, list_comments in dict_synthetic.items():\n",
    "            total_synthetic_considered += 1\n",
    "    \n",
    "            if reddit_json in bertscore_computed:\n",
    "                bertscore = 0\n",
    "                if synthetic_id in bertscore_computed[reddit_json]:\n",
    "                    bertscore = bertscore_computed[reddit_json][synthetic_id]['f1'][0]\n",
    "                #else:\n",
    "                    #print(list_comments)\n",
    "    \n",
    "                # check perplexity within the threshold\n",
    "                id_to_comments = dict()\n",
    "                if bertscore >= 0.5:\n",
    "                    total_synthetic_left += 1\n",
    "                    id_to_comments[synthetic_id] = list_comments\n",
    "                    list_dict.append(id_to_comments)\n",
    "                else:\n",
    "                    total_synthetic_filtered += 1\n",
    "                    id_to_comments[synthetic_id] = list_comments\n",
    "                    list_filtered.append(id_to_comments)\n",
    "        # saving to the file\n",
    "        added = save_jsonl(CONTENT_PRES_FILTER_DIR, reddit_json, list_dict, reddit_to_bertscore_data)\n",
    "    \n",
    "        # saving filtered data\n",
    "        added_filtered = save_jsonl(CONTENT_PRES_FILTERED_DIR, reddit_json, list_filtered, reddit_to_bertscore_filtered)\n",
    "        print(\"Number of new IDs that passed filter added: \" + str(added))\n",
    "        print(\"Number of new IDs that were filtered: \" + str(added_filtered))\n",
    "        print(\"-------------------\")\n",
    "\n",
    "        total_added += added\n",
    "        total_filtered += added_filtered\n",
    "    \n",
    "    print(\"Total number of synthetic comments considered: \" + str(total_synthetic_considered))\n",
    "    print(\"Total number of synthetic comments filtered: \" + str(total_synthetic_filtered))\n",
    "    print(\"% of the synthetic comments filtered: \" + str((total_synthetic_filtered) / total_synthetic_considered))\n",
    "    print(\"Total number of synthetic comments left: \" + str(total_synthetic_left))\n",
    "\n",
    "    print(\"total added: \" + str(total_added))\n",
    "    print(\"total filtered: \" + str(total_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c5deec9-0c60-4389-9ab0-1354e15eb8be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen_formality.jsonl\n",
      "Number of new IDs that passed filter added: 199845\n",
      "Number of new IDs that were filtered: 44560\n",
      "-------------------\n",
      "libertarian_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 171722\n",
      "Number of new IDs that were filtered: 40525\n",
      "-------------------\n",
      "askscience_humor.jsonl\n",
      "Number of new IDs that passed filter added: 210169\n",
      "Number of new IDs that were filtered: 16166\n",
      "-------------------\n",
      "askwomen_length.jsonl\n",
      "Number of new IDs that passed filter added: 182313\n",
      "Number of new IDs that were filtered: 48308\n",
      "-------------------\n",
      "askwomen_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 195901\n",
      "Number of new IDs that were filtered: 48164\n",
      "-------------------\n",
      "shittyaskscience_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 129351\n",
      "Number of new IDs that were filtered: 67866\n",
      "-------------------\n",
      "libertarian_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 162696\n",
      "Number of new IDs that were filtered: 49411\n",
      "-------------------\n",
      "askscience_length.jsonl\n",
      "Number of new IDs that passed filter added: 188298\n",
      "Number of new IDs that were filtered: 29569\n",
      "-------------------\n",
      "asksciencediscussion_length.jsonl\n",
      "Number of new IDs that passed filter added: 165558\n",
      "Number of new IDs that were filtered: 37943\n",
      "-------------------\n",
      "democrats_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 155103\n",
      "Number of new IDs that were filtered: 44332\n",
      "-------------------\n",
      "asktransgender_length.jsonl\n",
      "Number of new IDs that passed filter added: 154669\n",
      "Number of new IDs that were filtered: 34956\n",
      "-------------------\n",
      "asksciencediscussion_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 185977\n",
      "Number of new IDs that were filtered: 25787\n",
      "-------------------\n",
      "stocks_humor.jsonl\n",
      "Number of new IDs that passed filter added: 179564\n",
      "Number of new IDs that were filtered: 40074\n",
      "-------------------\n",
      "republican_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 172955\n",
      "Number of new IDs that were filtered: 47775\n",
      "-------------------\n",
      "libertarian_length.jsonl\n",
      "Number of new IDs that passed filter added: 149486\n",
      "Number of new IDs that were filtered: 53324\n",
      "-------------------\n",
      "asktransgender_formality.jsonl\n",
      "Number of new IDs that passed filter added: 180347\n",
      "Number of new IDs that were filtered: 14574\n",
      "-------------------\n",
      "shittyaskscience_humor.jsonl\n",
      "Number of new IDs that passed filter added: 128539\n",
      "Number of new IDs that were filtered: 68959\n",
      "-------------------\n",
      "wallstreetbets_humor.jsonl\n",
      "Number of new IDs that passed filter added: 151618\n",
      "Number of new IDs that were filtered: 71558\n",
      "-------------------\n",
      "stocks_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 177895\n",
      "Number of new IDs that were filtered: 41432\n",
      "-------------------\n",
      "askmen_humor.jsonl\n",
      "Number of new IDs that passed filter added: 180885\n",
      "Number of new IDs that were filtered: 64411\n",
      "-------------------\n",
      "pennystocks_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 172509\n",
      "Number of new IDs that were filtered: 57468\n",
      "-------------------\n",
      "democrats_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 148810\n",
      "Number of new IDs that were filtered: 51122\n",
      "-------------------\n",
      "wallstreetbetsnew_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 128150\n",
      "Number of new IDs that were filtered: 101982\n",
      "-------------------\n",
      "stocks_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 171775\n",
      "Number of new IDs that were filtered: 47573\n",
      "-------------------\n",
      "asksciencediscussion_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 179216\n",
      "Number of new IDs that were filtered: 32621\n",
      "-------------------\n",
      "asksciencediscussion_humor.jsonl\n",
      "Number of new IDs that passed filter added: 186043\n",
      "Number of new IDs that were filtered: 25836\n",
      "-------------------\n",
      "askmen_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 183054\n",
      "Number of new IDs that were filtered: 61919\n",
      "-------------------\n",
      "askwomen_formality.jsonl\n",
      "Number of new IDs that passed filter added: 217688\n",
      "Number of new IDs that were filtered: 25667\n",
      "-------------------\n",
      "wallstreetbetsnew_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 119553\n",
      "Number of new IDs that were filtered: 110183\n",
      "-------------------\n",
      "askwomen_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 203254\n",
      "Number of new IDs that were filtered: 40906\n",
      "-------------------\n",
      "asksciencediscussion_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 183895\n",
      "Number of new IDs that were filtered: 27869\n",
      "-------------------\n",
      "libertarian_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 168656\n",
      "Number of new IDs that were filtered: 43459\n",
      "-------------------\n",
      "shittyaskscience_formality.jsonl\n",
      "Number of new IDs that passed filter added: 148292\n",
      "Number of new IDs that were filtered: 48374\n",
      "-------------------\n",
      "shittyaskscience_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 128151\n",
      "Number of new IDs that were filtered: 69460\n",
      "-------------------\n",
      "democrats_humor.jsonl\n",
      "Number of new IDs that passed filter added: 156540\n",
      "Number of new IDs that were filtered: 44007\n",
      "-------------------\n",
      "asktransgender_humor.jsonl\n",
      "Number of new IDs that passed filter added: 173383\n",
      "Number of new IDs that were filtered: 22488\n",
      "-------------------\n",
      "democrats_length.jsonl\n",
      "Number of new IDs that passed filter added: 134862\n",
      "Number of new IDs that were filtered: 54149\n",
      "-------------------\n",
      "libertarian_humor.jsonl\n",
      "Number of new IDs that passed filter added: 168092\n",
      "Number of new IDs that were filtered: 41815\n",
      "-------------------\n",
      "republican_formality.jsonl\n",
      "Number of new IDs that passed filter added: 187380\n",
      "Number of new IDs that were filtered: 33107\n",
      "-------------------\n",
      "asktransgender_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 161960\n",
      "Number of new IDs that were filtered: 33529\n",
      "-------------------\n",
      "wallstreetbets_formality.jsonl\n",
      "Number of new IDs that passed filter added: 169110\n",
      "Number of new IDs that were filtered: 51698\n",
      "-------------------\n",
      "wallstreetbetsnew_length.jsonl\n",
      "Number of new IDs that passed filter added: 107643\n",
      "Number of new IDs that were filtered: 101532\n",
      "-------------------\n",
      "wallstreetbets_length.jsonl\n",
      "Number of new IDs that passed filter added: 117021\n",
      "Number of new IDs that were filtered: 88180\n",
      "-------------------\n",
      "pennystocks_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 166200\n",
      "Number of new IDs that were filtered: 63750\n",
      "-------------------\n",
      "wallstreetbetsnew_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 133363\n",
      "Number of new IDs that were filtered: 95507\n",
      "-------------------\n",
      "libertarian_formality.jsonl\n",
      "Number of new IDs that passed filter added: 182492\n",
      "Number of new IDs that were filtered: 27182\n",
      "-------------------\n",
      "wallstreetbets_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 144012\n",
      "Number of new IDs that were filtered: 79380\n",
      "-------------------\n",
      "askmen_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 182986\n",
      "Number of new IDs that were filtered: 62178\n",
      "-------------------\n",
      "republican_humor.jsonl\n",
      "Number of new IDs that passed filter added: 172390\n",
      "Number of new IDs that were filtered: 48153\n",
      "-------------------\n",
      "democrats_formality.jsonl\n",
      "Number of new IDs that passed filter added: 170726\n",
      "Number of new IDs that were filtered: 28641\n",
      "-------------------\n",
      "askwomen_humor.jsonl\n",
      "Number of new IDs that passed filter added: 204034\n",
      "Number of new IDs that were filtered: 40046\n",
      "-------------------\n",
      "pennystocks_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 175930\n",
      "Number of new IDs that were filtered: 53303\n",
      "-------------------\n",
      "pennystocks_formality.jsonl\n",
      "Number of new IDs that passed filter added: 194444\n",
      "Number of new IDs that were filtered: 33206\n",
      "-------------------\n",
      "asksciencediscussion_formality.jsonl\n",
      "Number of new IDs that passed filter added: 195486\n",
      "Number of new IDs that were filtered: 15814\n",
      "-------------------\n",
      "stocks_length.jsonl\n",
      "Number of new IDs that passed filter added: 154312\n",
      "Number of new IDs that were filtered: 53952\n",
      "-------------------\n",
      "askscience_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 198538\n",
      "Number of new IDs that were filtered: 27634\n",
      "-------------------\n",
      "askmen_length.jsonl\n",
      "Number of new IDs that passed filter added: 163692\n",
      "Number of new IDs that were filtered: 67223\n",
      "-------------------\n",
      "askscience_formality.jsonl\n",
      "Number of new IDs that passed filter added: 216581\n",
      "Number of new IDs that were filtered: 9083\n",
      "-------------------\n",
      "wallstreetbets_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 135828\n",
      "Number of new IDs that were filtered: 87127\n",
      "-------------------\n",
      "wallstreetbets_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 146655\n",
      "Number of new IDs that were filtered: 75753\n",
      "-------------------\n",
      "asktransgender_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 170633\n",
      "Number of new IDs that were filtered: 24868\n",
      "-------------------\n",
      "republican_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 157982\n",
      "Number of new IDs that were filtered: 62877\n",
      "-------------------\n",
      "pennystocks_length.jsonl\n",
      "Number of new IDs that passed filter added: 147892\n",
      "Number of new IDs that were filtered: 66017\n",
      "-------------------\n",
      "wallstreetbetsnew_humor.jsonl\n",
      "Number of new IDs that passed filter added: 136834\n",
      "Number of new IDs that were filtered: 93018\n",
      "-------------------\n",
      "democrats_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 155965\n",
      "Number of new IDs that were filtered: 44473\n",
      "-------------------\n",
      "shittyaskscience_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 121562\n",
      "Number of new IDs that were filtered: 75791\n",
      "-------------------\n",
      "republican_length.jsonl\n",
      "Number of new IDs that passed filter added: 149456\n",
      "Number of new IDs that were filtered: 60097\n",
      "-------------------\n",
      "askscience_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 208199\n",
      "Number of new IDs that were filtered: 17984\n",
      "-------------------\n",
      "stocks_formality.jsonl\n",
      "Number of new IDs that passed filter added: 191458\n",
      "Number of new IDs that were filtered: 26752\n",
      "-------------------\n",
      "askscience_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 206829\n",
      "Number of new IDs that were filtered: 19371\n",
      "-------------------\n",
      "pennystocks_humor.jsonl\n",
      "Number of new IDs that passed filter added: 181040\n",
      "Number of new IDs that were filtered: 48794\n",
      "-------------------\n",
      "wallstreetbetsnew_formality.jsonl\n",
      "Number of new IDs that passed filter added: 158159\n",
      "Number of new IDs that were filtered: 68949\n",
      "-------------------\n",
      "shittyaskscience_length.jsonl\n",
      "Number of new IDs that passed filter added: 105091\n",
      "Number of new IDs that were filtered: 78638\n",
      "-------------------\n",
      "asktransgender_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 169197\n",
      "Number of new IDs that were filtered: 26524\n",
      "-------------------\n",
      "stocks_politeness.jsonl\n",
      "Number of new IDs that passed filter added: 177839\n",
      "Number of new IDs that were filtered: 41788\n",
      "-------------------\n",
      "askmen_supportiveness.jsonl\n",
      "Number of new IDs that passed filter added: 170776\n",
      "Number of new IDs that were filtered: 74657\n",
      "-------------------\n",
      "republican_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 165606\n",
      "Number of new IDs that were filtered: 54681\n",
      "-------------------\n",
      "askwomen_sarcasm.jsonl\n",
      "Number of new IDs that passed filter added: 205532\n",
      "Number of new IDs that were filtered: 38361\n",
      "-------------------\n",
      "Total number of synthetic comments considered: 16951857\n",
      "Total number of synthetic comments filtered: 3896210\n",
      "% of the synthetic comments filtered: 0.22983971608538226\n",
      "Total number of synthetic comments left: 13055647\n",
      "total added: 13055647\n",
      "total filtered: 3896210\n"
     ]
    }
   ],
   "source": [
    "# only execute cell once all bertscore has been computed (e.g. GPU jobs are done)\n",
    "apply_content_preservation_filter()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
