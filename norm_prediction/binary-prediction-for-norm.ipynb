{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c758374",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stellali/miniconda3/envs/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9bc03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2b91f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "# MODEL_NAME = \"microsoft/DialoGPT-large\"\n",
    "MODEL_NAME = \"microsoft/DialoGPT-medium\"\n",
    "# MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "# MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "def initialize_model(modelname, bool_tokenizer=False, bool_model=True):\n",
    "    if bool_tokenizer:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(modelname, truncation=True, max_len=MAX_LEN, padding='max_length', cache_dir=\"/Users/stellali/repos/reddit_norm/.cache\")\n",
    "    else:\n",
    "        tokenizer = None\n",
    "    if bool_model:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(modelname, num_labels=2, cache_dir=\"/Users/stellali/repos/reddit_norm/.cache\").to(device)\n",
    "    else:\n",
    "        model = None\n",
    "    if tokenizer and tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    if model and \"DialoGPT\" in modelname:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    return model, tokenizer\n",
    "_, tokenizer = initialize_model(MODEL_NAME, bool_tokenizer=True, bool_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c0f9367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/'"
     ]
    }
   ],
   "source": [
    "os.listdir(\"../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a88ea7",
   "metadata": {},
   "source": [
    "# Loading and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f4780f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw 100 synthetic 1250\n"
     ]
    }
   ],
   "source": [
    "path_data = \"../data/\"\n",
    "with open(path_data+\"aggregated_binary_annotations_0_4.pkl\",\"rb\") as file:\n",
    "    raw_data = pickle.load(file)\n",
    "raw_data = pd.DataFrame(raw_data)\n",
    "\n",
    "NORM_DIMENSION=\"formality\"\n",
    "path_synthetic_file = {'supportiveness':'gender_supportive_toxic_synthetic_data.csv',\n",
    "                      'sarcasm': 'gender_genuine_sarcasm_synthetic_data.csv',\n",
    "                       'politeness': 'gender_rude_polite_synthetic_data.csv',\n",
    "                       'humorous': 'gender_humor_serious_synthetic_data.csv',\n",
    "                       'formality': 'gender_casual_formal_synthetic_data.csv'\n",
    "                      }\n",
    "\n",
    "synthetic = pd.read_csv(path_data+path_synthetic_file[NORM_DIMENSION])\n",
    "synthetic = synthetic.rename({'title1': 'post_title_1', 'description1':'post_description_1', 'comment1':'comment_1',\n",
    "                              'title2': 'post_title_2', 'description2':'post_description_2', 'comment2':'comment_2'}, axis=1)\n",
    "synthetic_old = None\n",
    "if NORM_DIMENSION == \"supportiveness\":\n",
    "    synthetic_old = pd.read_csv(path_data+\"gender_supportive_toxic_synthetic_data_old.csv\")\n",
    "    synthetic_old = synthetic_old.rename({'title1': 'post_title_1', 'description1':'post_description_1', 'comment1':'comment_1',\n",
    "                                  'title2': 'post_title_2', 'description2':'post_description_2', 'comment2':'comment_2'}, axis=1)\n",
    "\n",
    "print(\"raw\", len(raw_data), \"synthetic\", len(synthetic))\n",
    "if synthetic_old:\n",
    "    print(\"synthetic_old\", len(synthetic_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bbe51db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['post_title_1', 'post_description_1', 'comment_1', 'post_title_2',\n",
      "       'post_description_2', 'comment_2', 'comment_metadata_1',\n",
      "       'post_metadata_1', 'comment_metadat2', 'post_metadata2', 'annotation'],\n",
      "      dtype='object')\n",
      "label not either 1 or 2 (label: tie)\n",
      "label not either 1 or 2 (label: None)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(raw_data.columns)\n",
    "\n",
    "def process_input_text(d, reverse_label=False):\n",
    "    \n",
    "    input1 = d['post_title_1'] +\"\\n\"+ d['post_description_1'] if type(d['post_description_1']) == str else d['post_title_1']\n",
    "#     input1 += \"\\nCOMMENT: \"+d['comment_1']\n",
    "#     comment1 = \"COMMENT1: \"+d['comment_1']\n",
    "    input2 = d['post_title_2'] +\"\\n\"+ d['post_description_2'] if type(d['post_description_2']) == str else d['post_title_2']\n",
    "#     input2 += \"\\nCOMMENT: \"+d['comment_2']\n",
    "    if reverse_label:\n",
    "        return \"Comment 1: \"+ d['comment_2']+\"\\nComment 2: \"+ d['comment_1'] + \"\\nPOST1: \"+input2+\"\\nPOST2: \"+input1\n",
    "    else:\n",
    "        return \"Comment 1: \"+ d['comment_1']+\"\\nComment 2: \"+ d['comment_2'] + \"\\nPOST1: \"+input1+\"\\nPOST2: \"+input2\n",
    "    \n",
    "def process_data(data:list[dict], balance_label=True)->list[str]:\n",
    "    '''process each sample to training data'''\n",
    "    if data is None:\n",
    "        return None\n",
    "    processed = []\n",
    "    num_labels = [0, 0]\n",
    "    for _, d in data.iterrows():\n",
    "        if 'annotation' in d:\n",
    "            try:\n",
    "                label = int(d['annotation'][NORM_DIMENSION]) -1\n",
    "            except:\n",
    "                print(f\"label not either 1 or 2 (label: {d['annotation'][NORM_DIMENSION]})\")\n",
    "                continue\n",
    "        elif 'synthetic_label' in d:\n",
    "            try:\n",
    "                label = int(d['synthetic_label']) -1\n",
    "            except:\n",
    "                print(f\"label not either 1 or 2\")\n",
    "                print(d['synthetic_label'])\n",
    "                continue\n",
    "        if balance_label:\n",
    "            less_num_label = 0 if num_labels[0] < num_labels[1] else 1\n",
    "            if less_num_label == label:\n",
    "#                 input_text = input1+input2\n",
    "                input_text = process_input_text(d)\n",
    "                instance = [input_text, less_num_label]\n",
    "            else:\n",
    "                input_text = process_input_text(d, reverse_label=True)\n",
    "                instance = [input_text, less_num_label]\n",
    "            num_labels[less_num_label] += 1\n",
    "        else:\n",
    "            input_text = process_input_text(d)\n",
    "            instance = [input_text, label]\n",
    "        tokenized = tokenizer(instance[0], max_length=MAX_LEN, padding = 'max_length', truncation=True, return_tensors='pt')\n",
    "#         tokenized = tokenizer(instance[0], max_length=MAX_LEN, padding = 'max_length', truncation=True, return_tensors='pt').to(device)\n",
    "        if 'token_type_ids' in tokenized:\n",
    "            tokenized = {'input_ids':tokenized['input_ids'][0], 'token_type_ids':tokenized['token_type_ids'][0], 'attention_mask':tokenized['attention_mask'][0]}\n",
    "        else:\n",
    "            tokenized = {'input_ids':tokenized['input_ids'][0], 'attention_mask':tokenized['attention_mask'][0]}\n",
    "        instance[0]=tokenized\n",
    "        processed.append(instance)\n",
    "    return processed\n",
    "\n",
    "data_human = process_data(raw_data)\n",
    "data_synthetic = process_data(synthetic)\n",
    "data_synthetic_old = process_data(synthetic_old)\n",
    "# data = []\n",
    "# for rdata in [raw_data, synthetic]:\n",
    "#   data += process_data(rdata)\n",
    "#   # print(data[0])\n",
    "#   print(\"size\", len(data))\n",
    "#   print(\"label distribution\", Counter([p[1] for p in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe827ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class CustomData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33995444",
   "metadata": {},
   "source": [
    "# training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe2c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_SAVE\n",
    "str_modelname=MODEL_NAME.split(\"/\")[-1]\n",
    "PATH_MODEL_SAVE=\"/mmfs1/home/chanyoun/models/\"+f\"{NORM_DIMENSION}-{str_modelname}/\"\n",
    "if not os.path.isdir(PATH_MODEL_SAVE):\n",
    "    os.mkdir(PATH_MODEL_SAVE)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(PATH_MODEL_SAVE+\"training.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6a25025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size\ttrain:1125\tdev:125\ttest:98\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "TEST_RATIO=0.2\n",
    "VAL_RATIO=0.1\n",
    "\n",
    "# random.shuffle(data)\n",
    "# num_test = int(len(data)*TEST_RATIO)\n",
    "# print(f\"data size\\ttrain:{len(data)-num_test}\\ttest:{num_test}\")\n",
    "# dataloader_train = DataLoader(CustomData(data[num_test:]), batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "# dataloader_test = DataLoader(CustomData(data[:num_test]), batch_size=VALID_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# data_human\n",
    "# data_synthetic\n",
    "if data_synthetic_old is not None:\n",
    "    data_synthetic_and_old = data_synthetic+data_synthetic_old\n",
    "    random.shuffle(data_synthetic_and_old)\n",
    "    num_val = int(len(data_synthetic_and_old)*VAL_RATIO)\n",
    "    data_dev, data_train  = data_synthetic_and_old[:num_val], data_synthetic_and_old[num_val:]\n",
    "    print(f\"data size\\ttrain:{len(data_train)}\\tdev:{len(data_dev)}\\ttest:{len(data_human)}\")\n",
    "    dataloader_train = DataLoader(CustomData(data_train), batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    dataloader_dev = DataLoader(CustomData(data_dev), batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    dataloader_test = DataLoader(CustomData(data_human), batch_size=VALID_BATCH_SIZE, shuffle=True)\n",
    "else:\n",
    "    random.shuffle(data_synthetic)\n",
    "    num_val = int(len(data_synthetic)*VAL_RATIO)\n",
    "    data_dev, data_train  = data_synthetic[:num_val], data_synthetic[num_val:]\n",
    "    print(f\"data size\\ttrain:{len(data_train)}\\tdev:{len(data_dev)}\\ttest:{len(data_human)}\")\n",
    "    dataloader_train = DataLoader(CustomData(data_train), batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    dataloader_dev = DataLoader(CustomData(data_dev), batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    dataloader_test = DataLoader(CustomData(data_human), batch_size=VALID_BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fe588a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloder):\n",
    "    model.eval()\n",
    "    test_loss, num_example, num_correct = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloder:\n",
    "            input, lbl = batch\n",
    "            input = {k:v.to(device) for k,v in input.items()}\n",
    "            lbl = lbl.to(device)\n",
    "            outputs = model(**input, labels=lbl)\n",
    "            test_loss += outputs.loss\n",
    "            num_example += len(lbl)\n",
    "            _, pred_idx = outputs.logits.max(dim=1)\n",
    "            num_correct += sum(pred_idx == lbl).item()\n",
    "    return test_loss/len(dataloder), num_correct/num_example\n",
    "\n",
    "\n",
    "def train(epoch=0, best_val_acc=0, best_test_acc=0, bool_verbose=True, bool_save=True, min_save_epoch=3):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_total_batch = len(dataloader_train)\n",
    "    num_batch, num_example, num_correct_pred = 0, 0, 0\n",
    "    for idx_batch, batch in enumerate(dataloader_train):\n",
    "        input, lbl = batch\n",
    "        input = {k:v.to(device) for k,v in input.items()}\n",
    "        lbl = lbl.to(device)\n",
    "        outputs = model(**input, labels=lbl)\n",
    "        train_loss += outputs.loss\n",
    "        num_example += len(lbl)\n",
    "        num_batch += 1\n",
    "        _, pred_idx = outputs.logits.max(dim=1)\n",
    "        num_correct_pred += sum(pred_idx == lbl).item()\n",
    "        if (idx_batch+1) % REPORT_EVERY == 0:\n",
    "            avg_train_loss = train_loss / num_batch\n",
    "            avg_acc = num_correct_pred / num_example\n",
    "            avg_val_loss, avg_val_acc = evaluate(dataloader_dev)\n",
    "            avg_test_loss, avg_test_acc = evaluate(dataloader_test)\n",
    "            if bool_verbose:\n",
    "                logging.info(f\"Epoch {epoch} [{idx_batch+1:>3}/{num_total_batch:<3}] loss: {avg_train_loss.item():<8.3f}acc: {avg_acc:<8.3f}val loss: {avg_val_loss:<8.3f}val acc: {avg_val_acc:<8.3f}test loss: {avg_test_loss:<8.3f}test acc: {avg_test_acc:<8.3f}\")\n",
    "#                 print(f\"Epoch {epoch} [{idx_batch+1:>3}/{num_total_batch:<3}] loss: {avg_train_loss.item():<8.3f}acc: {avg_acc:<8.3f}val loss: {avg_val_loss:<8.3f}val acc: {avg_val_acc:<8.3f}test loss: {avg_test_loss:<8.3f}test acc: {avg_test_acc:<8.3f}\")\n",
    "            if avg_val_acc > best_val_acc :\n",
    "                best_val_acc = avg_val_acc\n",
    "                best_test_acc = avg_test_acc\n",
    "                if bool_verbose:\n",
    "                    logging.info(f\"new best val acc found ({avg_val_acc:.3f}), new test acc: {avg_test_acc:.3f}\")\n",
    "#                     print(f\"new best val acc found ({avg_val_acc:.3f}), new test acc: {avg_test_acc:.3f}\")\n",
    "                if bool_save and epoch>=min_save_epoch:\n",
    "                    logging.info(f\"saving the model to {PATH_MODEL_SAVE}\")\n",
    "#                     print(f\"saving the model to {PATH_MODEL_SAVE}\")\n",
    "                    model.save_pretrained(PATH_MODEL_SAVE, from_pt=True) \n",
    "            train_loss, num_batch, num_correct_pred, num_example = 0, 0, 0, 0\n",
    "            model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs.loss.backward()\n",
    "        optimizer.step()\n",
    "    return best_val_acc, best_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb8d360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cecd29c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Free:47.333GB\tAvailable:47.619GB\n"
     ]
    }
   ],
   "source": [
    "get_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a531745",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at microsoft/DialoGPT-medium and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-04-09 21:44:24,222 [INFO] Epoch 1 [ 70/282] loss: 0.724   acc: 0.493   val loss: 0.710   val acc: 0.496   test loss: 0.700   test acc: 0.439   \n",
      "2024-04-09 21:44:24,231 [INFO] new best val acc found (0.496), new test acc: 0.439\n",
      "2024-04-09 21:45:48,863 [INFO] Epoch 1 [140/282] loss: 0.705   acc: 0.500   val loss: 0.684   val acc: 0.552   test loss: 0.682   test acc: 0.531   \n",
      "2024-04-09 21:45:48,864 [INFO] new best val acc found (0.552), new test acc: 0.531\n",
      "2024-04-09 21:47:13,527 [INFO] Epoch 1 [210/282] loss: 0.707   acc: 0.500   val loss: 0.688   val acc: 0.472   test loss: 0.696   test acc: 0.449   \n",
      "2024-04-09 21:48:38,256 [INFO] Epoch 1 [280/282] loss: 0.688   acc: 0.546   val loss: 0.689   val acc: 0.528   test loss: 0.724   test acc: 0.459   \n",
      "2024-04-09 21:50:04,383 [INFO] Epoch 2 [ 70/282] loss: 0.690   acc: 0.546   val loss: 0.671   val acc: 0.616   test loss: 0.700   test acc: 0.510   \n",
      "2024-04-09 21:50:04,385 [INFO] new best val acc found (0.616), new test acc: 0.510\n",
      "2024-04-09 21:51:28,917 [INFO] Epoch 2 [140/282] loss: 0.700   acc: 0.539   val loss: 0.699   val acc: 0.536   test loss: 0.709   test acc: 0.459   \n",
      "2024-04-09 21:52:53,515 [INFO] Epoch 2 [210/282] loss: 0.690   acc: 0.579   val loss: 0.689   val acc: 0.536   test loss: 0.729   test acc: 0.418   \n",
      "2024-04-09 21:54:18,116 [INFO] Epoch 2 [280/282] loss: 0.693   acc: 0.557   val loss: 0.710   val acc: 0.480   test loss: 0.721   test acc: 0.490   \n",
      "2024-04-09 21:55:44,202 [INFO] Epoch 3 [ 70/282] loss: 0.663   acc: 0.643   val loss: 0.679   val acc: 0.560   test loss: 0.732   test acc: 0.510   \n",
      "2024-04-09 21:57:08,765 [INFO] Epoch 3 [140/282] loss: 0.650   acc: 0.650   val loss: 0.655   val acc: 0.632   test loss: 0.703   test acc: 0.469   \n",
      "2024-04-09 21:57:08,766 [INFO] new best val acc found (0.632), new test acc: 0.469\n",
      "2024-04-09 21:58:33,298 [INFO] Epoch 3 [210/282] loss: 0.686   acc: 0.586   val loss: 0.674   val acc: 0.504   test loss: 0.710   test acc: 0.531   \n",
      "2024-04-09 21:59:57,916 [INFO] Epoch 3 [280/282] loss: 0.654   acc: 0.625   val loss: 0.672   val acc: 0.576   test loss: 0.710   test acc: 0.500   \n",
      "2024-04-09 22:01:23,974 [INFO] Epoch 4 [ 70/282] loss: 0.654   acc: 0.607   val loss: 0.708   val acc: 0.488   test loss: 0.728   test acc: 0.490   \n"
     ]
    }
   ],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "NUM_EPOCHS = 15\n",
    "REPORT_EVERY = 70\n",
    "LEARNING_RATE = 1e-05\n",
    "WEIGHT_DECAY= 1e-04\n",
    "\n",
    "# optimizer\n",
    "# model, _ = initialize_model(\"microsoft/deberta-v3-large\")\n",
    "# model, _ = initialize_model(\"microsoft/deberta-v3-base\")\n",
    "model, _ = initialize_model(MODEL_NAME, bool_tokenizer=False)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "best_val_acc, best_test_acc = 0, 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    best_val_acc, best_test_acc = train(epoch+1, best_val_acc, best_test_acc, bool_save=False)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "503b7f30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [140/563] loss: 0.701   acc: 0.487   val loss: 0.694   val acc: 0.494   test loss: 0.695   test acc: 0.475   \n",
      "new best val acc found (0.494), new test acc: 0.475\n",
      "Epoch 1 [280/563] loss: 0.693   acc: 0.521   val loss: 0.692   val acc: 0.498   test loss: 0.692   test acc: 0.515   \n",
      "new best val acc found (0.498), new test acc: 0.515\n",
      "Epoch 1 [420/563] loss: 0.692   acc: 0.537   val loss: 0.668   val acc: 0.602   test loss: 0.660   test acc: 0.596   \n",
      "new best val acc found (0.602), new test acc: 0.596\n",
      "Epoch 1 [560/563] loss: 0.630   acc: 0.686   val loss: 0.540   val acc: 0.743   test loss: 0.605   test acc: 0.667   \n",
      "new best val acc found (0.743), new test acc: 0.667\n",
      "Epoch 2 [140/563] loss: 0.505   acc: 0.780   val loss: 0.404   val acc: 0.827   test loss: 0.522   test acc: 0.768   \n",
      "new best val acc found (0.827), new test acc: 0.768\n",
      "Epoch 2 [280/563] loss: 0.432   acc: 0.804   val loss: 0.431   val acc: 0.795   test loss: 0.569   test acc: 0.768   \n",
      "Epoch 2 [420/563] loss: 0.385   acc: 0.832   val loss: 0.364   val acc: 0.831   test loss: 0.516   test acc: 0.717   \n",
      "new best val acc found (0.831), new test acc: 0.717\n",
      "Epoch 2 [560/563] loss: 0.367   acc: 0.839   val loss: 0.343   val acc: 0.851   test loss: 0.560   test acc: 0.737   \n",
      "new best val acc found (0.851), new test acc: 0.737\n",
      "Epoch 3 [140/563] loss: 0.312   acc: 0.873   val loss: 0.272   val acc: 0.904   test loss: 0.600   test acc: 0.677   \n",
      "new best val acc found (0.904), new test acc: 0.677\n",
      "saving the model to /mmfs1/home/chanyoun/models\n",
      "Epoch 3 [280/563] loss: 0.263   acc: 0.887   val loss: 0.400   val acc: 0.855   test loss: 0.514   test acc: 0.778   \n",
      "Epoch 3 [420/563] loss: 0.290   acc: 0.887   val loss: 0.344   val acc: 0.859   test loss: 0.469   test acc: 0.758   \n",
      "Epoch 3 [560/563] loss: 0.219   acc: 0.921   val loss: 0.382   val acc: 0.859   test loss: 0.568   test acc: 0.788   \n",
      "Epoch 4 [140/563] loss: 0.171   acc: 0.936   val loss: 0.353   val acc: 0.880   test loss: 0.600   test acc: 0.788   \n",
      "Epoch 4 [280/563] loss: 0.178   acc: 0.918   val loss: 0.297   val acc: 0.884   test loss: 0.459   test acc: 0.818   \n",
      "Epoch 4 [420/563] loss: 0.214   acc: 0.900   val loss: 0.312   val acc: 0.871   test loss: 0.534   test acc: 0.768   \n",
      "Epoch 4 [560/563] loss: 0.234   acc: 0.898   val loss: 0.321   val acc: 0.876   test loss: 0.567   test acc: 0.737   \n",
      "Epoch 5 [140/563] loss: 0.092   acc: 0.966   val loss: 0.315   val acc: 0.884   test loss: 0.512   test acc: 0.828   \n",
      "Epoch 5 [280/563] loss: 0.129   acc: 0.955   val loss: 0.302   val acc: 0.892   test loss: 0.500   test acc: 0.828   \n",
      "Epoch 5 [420/563] loss: 0.171   acc: 0.929   val loss: 0.378   val acc: 0.884   test loss: 0.652   test acc: 0.798   \n",
      "Epoch 5 [560/563] loss: 0.182   acc: 0.934   val loss: 0.389   val acc: 0.863   test loss: 0.479   test acc: 0.828   \n",
      "Epoch 6 [140/563] loss: 0.123   acc: 0.946   val loss: 0.410   val acc: 0.859   test loss: 0.564   test acc: 0.778   \n",
      "Epoch 6 [280/563] loss: 0.097   acc: 0.955   val loss: 0.477   val acc: 0.888   test loss: 0.752   test acc: 0.788   \n",
      "Epoch 6 [420/563] loss: 0.100   acc: 0.964   val loss: 0.528   val acc: 0.884   test loss: 0.838   test acc: 0.788   \n",
      "Epoch 6 [560/563] loss: 0.128   acc: 0.950   val loss: 0.406   val acc: 0.884   test loss: 0.668   test acc: 0.778   \n",
      "Epoch 7 [140/563] loss: 0.100   acc: 0.963   val loss: 0.408   val acc: 0.896   test loss: 0.567   test acc: 0.818   \n",
      "Epoch 7 [280/563] loss: 0.066   acc: 0.975   val loss: 0.329   val acc: 0.888   test loss: 0.696   test acc: 0.798   \n",
      "Epoch 7 [420/563] loss: 0.099   acc: 0.957   val loss: 0.364   val acc: 0.896   test loss: 0.684   test acc: 0.798   \n",
      "Epoch 7 [560/563] loss: 0.145   acc: 0.945   val loss: 0.300   val acc: 0.920   test loss: 0.639   test acc: 0.737   \n",
      "new best val acc found (0.920), new test acc: 0.737\n",
      "saving the model to /mmfs1/home/chanyoun/models\n",
      "Epoch 8 [140/563] loss: 0.044   acc: 0.984   val loss: 0.344   val acc: 0.900   test loss: 0.722   test acc: 0.828   \n",
      "Epoch 8 [280/563] loss: 0.062   acc: 0.977   val loss: 0.352   val acc: 0.892   test loss: 0.675   test acc: 0.828   \n",
      "Epoch 8 [420/563] loss: 0.101   acc: 0.961   val loss: 0.405   val acc: 0.867   test loss: 0.780   test acc: 0.818   \n",
      "Epoch 8 [560/563] loss: 0.076   acc: 0.970   val loss: 0.510   val acc: 0.871   test loss: 0.627   test acc: 0.828   \n"
     ]
    }
   ],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "NUM_EPOCHS = 8\n",
    "REPORT_EVERY = 140\n",
    "LEARNING_RATE = 1e-05\n",
    "WEIGHT_DECAY= 1e-04\n",
    "\n",
    "# optimizer\n",
    "model, _ = initialize_model(\"microsoft/deberta-v3-large\")\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "best_val_acc, best_test_acc = 0, 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    best_val_acc, best_test_acc = train(epoch+1, best_val_acc, best_test_acc)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
